<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"rennesong.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Personal blog site, to note technique and life related articles">
<meta property="og:type" content="website">
<meta property="og:title" content="Rennesong&#39;s blog">
<meta property="og:url" content="http://rennesong.com/page/2/index.html">
<meta property="og:site_name" content="Rennesong&#39;s blog">
<meta property="og:description" content="Personal blog site, to note technique and life related articles">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="rennsong">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://rennesong.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Rennesong's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Rennesong's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Blog my life from now on!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/10/27/python-regex/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/27/python-regex/" class="post-title-link" itemprop="url">正则表达式以及在 Python 中的使用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-27 19:02:08" itemprop="dateCreated datePublished" datetime="2019-10-27T19:02:08+08:00">2019-10-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-06 05:35:50" itemprop="dateModified" datetime="2019-11-06T05:35:50+08:00">2019-11-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/Regluar-expression/" itemprop="url" rel="index"><span itemprop="name">Regluar expression</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="关于本文的定位">关于本文的定位</h1>
<p>系统地介绍正则表达式规则，使用不是一篇博文可以涵盖的。基本的概念和知识可以通过网上的文章去了解。同时正则表达式在不同的编程语言下，其实现与使用略有不同。本文只针对 Python中的正则表达式。本文的主要目的是，记录和总结自己在平时的工作中，通过一些具体的用例，学到的一些 Python 正则表达式的技巧，以备不时之需。所以可以认为本文是基于 case driven 的理念。</p>
<h2 id="section"></h2>
<p>方案: 采用(?!...) Matches if ... doesn’t match next. This is a negative lookahead assertion. For example, Isaac (?!Asimov) will match 'Isaac ' only if it’s not followed by 'Asimov'.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/10/26/leetcode-notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/26/leetcode-notes/" class="post-title-link" itemprop="url">Leetcode 刷题心得</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-26 18:30:33" itemprop="dateCreated datePublished" datetime="2019-10-26T18:30:33+08:00">2019-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-06 05:35:53" itemprop="dateModified" datetime="2019-11-06T05:35:53+08:00">2019-11-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>原来Python的list只要是空，那么在if语句中就可以视为false了 假定一个输入的list是参数a if not a 其实可以涵盖两种情况了: a是一个empty list以及a是一个None.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/10/15/linux-c-learning-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/15/linux-c-learning-note/" class="post-title-link" itemprop="url">linux-c-learning-note</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-15 21:33:34" itemprop="dateCreated datePublished" datetime="2019-10-15T21:33:34+08:00">2019-10-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-10-16 22:07:55" itemprop="dateModified" datetime="2019-10-16T22:07:55+08:00">2019-10-16</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Linux C编程一站式学习 读书心得</p>
<h2 id="计算机数字表示">计算机数字表示</h2>
<h3 id="sign-and-magnitude-方式">Sign and magnitude 方式</h3>
<ul>
<li>原理: 最高有效位表示符号，其余比特表示数字的绝对值。</li>
<li>优势：简单易懂，符合人们的思维</li>
<li>缺点：1）计算机做运算的时候效率不高 2）同时需要加减法电路，3）零的表示有两种</li>
</ul>
<h3 id="的补码表示法">1的补码表示法</h3>
<ul>
<li>原理: 正数不变，负数用1的补码（1's Complement）表示，减法转换成加法，计算结果的最高位如果有进位则要加回到最低位上去。取1的补码就是把每个bit取反</li>
<li>优势：相较于 sign and magnitude 表示法，不需要把符号和绝对值分开考虑，正数和负数的加法都一样算，计算逻辑更简单硬件电路大大简化(无需减法电路)</li>
<li>缺点：0的表示仍然不唯一</li>
</ul>
<h3 id="的补码表示法-1">2的补码表示法</h3>
<ul>
<li>原理：正数不变，负数先取反码再加1。如果8个bit采用2's Complement表示法，负数的取值范围是从10000000到11111111（-128<sub>-1），正数是从00000000到01111111（0</sub>127），也可以根据最高位判断一个数是正是负，并且0的表示是唯一的，目前绝大多数计算机都采用这种表示法</li>
<li>优点：在1的补码表示法的基础上，保证0的表示方法唯一</li>
<li>缺点：相对难理解一些</li>
</ul>
<h3 id="溢出问题">溢出问题</h3>
<ul>
<li>在相加过程中最高位产生的进位和次高位产生的进位如果相同则没有溢出，如果不同则表示有溢出。</li>
<li>计算机的加法器在做完计算之后，根据最高位产生的进位设置进位标志，同时根据最高位和次高位产生的进位的异或设置溢出标志。</li>
<li>浮点数在计算机中的表示是基于科学计数法（Scientific Notation), 浮点数表示是一个相当复杂的话题，不同平台的浮点数表示和浮点运算也有较大差异，</li>
</ul>
<h3 id="数据类型详解">数据类型详解</h3>
<p>除了char型以外的这些类型如果不明确写signed或unsigned关键字都表示signed，这一点是C标准明确规定的，不是Implementation Defined。</p>
<p>除了char型在C标准中明确规定占一个字节之外，其它整型占几个字节都是Implementation Defined。通常的编译器实现遵守ILP32或LP64规范。</p>
<p>ILP32这个缩写的意思是int（I）、long（L）和指针（P）类型都占32位，通常32位计算机的C编译器采用这种规范，x86平台的gcc也是如此。</p>
<p>LP64是指long（L）和指针占64位，通常64位计算机的C编译器采用这种规范。指针类型的长度总是和计算机的位数一致，</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/10/14/voip-tuto/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/14/voip-tuto/" class="post-title-link" itemprop="url">voip-tuto</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-14 02:44:12" itemprop="dateCreated datePublished" datetime="2019-10-14T02:44:12+08:00">2019-10-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-10-16 22:07:49" itemprop="dateModified" datetime="2019-10-16T22:07:49+08:00">2019-10-16</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="voip-traffic-simulation-with-iperf">VoIP traffic simulation with Iperf</h1>
<p>source: https://www.onsip.com/voip-resources/voip-fundamentals/iperf-simulating-voip-traffic-ubernerds</p>
<p>When testing components of a VOIP platform, Iperf can be quite handy. Quoting from the Iperf documentation, &quot;Iperf creates a constant bit rate UDP stream. This is a very artificial stream, similar to voice communication but not much else.&quot; While it operates in TCP mode by default, simply adding the UDP flag (-u) to all the commands will flip Iperf into a more suitable mode and adjusting the datagram size (-l) to 250 gives a close approximation to 20ms g711.</p>
<p>Run Iperf on one system in server mode (-s) and on another in client mode (-c) and within seconds it starts spitting out nice reports on bandwidth, jitter, and loss. Finding the upper limit of system and network components is as simple turning up some of the Iperf dials (-b and -S among others) and waiting for jitter and dropped packets to show up. While it cannot provide the whole picture, combine Iperf data with the output of system monitoring tools like vmstat and an empirical baseline of how a system will behave in the face of VOIP traffic begins to materialize. So I recommend considering Iperf next time you are adding to your UberNerd toolbox.</p>
<h2 id="how-to-evaluate-voip-packet-size">How to evaluate VoIP packet size?</h2>
<p>https://dev.to/onmyway133/how-to-calculate-packet-size-in-voip--54ac</p>
<h2 id="iperf-installation">iperf installation</h2>
<p>You may find that, the iperf package shipped with your linux distribution is the most recent one. For my case, the installed iperf's version via apt utility is 2.0.5, however, to enable enhanced iperf measure (e.g. with end-to-end latency), iperf 2.0.9+ is required.</p>
<p>We could download DEB package and manually install DEB package to override the older version of iperf.</p>
<p>The URL to download DEB package: http://archive.ubuntu.com/ubuntu/pool/universe/i/iperf/iperf_2.0.10+dfsg1-1ubuntu0.18.04.2_amd64.deb</p>
<p>If you use iperf 2.0.10. At server side <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iperf -i1 -u -s -p 5003 -e</span><br><span class="line">docker exec -it classifier2 ip netns exec app iperf -i1 -u -s -p 5003 -e</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iperf -c 192.168.2.2 -u -i1 -p 5003 -l 250 -b 64K --enhanced</span><br><span class="line">docker exec -it classifier1 ip netns exec app iperf -c 192.168.2.2 -u -i1 -p 5003 -l 250 -b 64K --enhanced</span><br></pre></td></tr></table></figure>
<p>def get_service_function_acl_uri(): return &quot;/restconf/config/ietf-access-control-list:access-lists/&quot;</p>
<p>def get_service_function_acl_data(rsp_name, rsp_rev_name): return { &quot;access-lists&quot;: { &quot;acl&quot;: [ { &quot;acl-name&quot;: &quot;ACL1&quot;, &quot;acl-type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;, &quot;access-list-entries&quot;: { &quot;ace&quot;: [ { &quot;rule-name&quot;: &quot;ACE1&quot;, &quot;actions&quot;: { &quot;service-function-acl:rendered-service-path&quot;: rsp_name }, &quot;matches&quot;: { &quot;destination-ipv4-network&quot;: &quot;192.168.2.0/24&quot;, &quot;source-ipv4-network&quot;: &quot;192.168.2.0/24&quot;, &quot;protocol&quot;: &quot;6&quot; } } ] } }, { &quot;acl-name&quot;: &quot;ACL1-ping&quot;, &quot;acl-type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;, &quot;access-list-entries&quot;: { &quot;ace&quot;: [ { &quot;rule-name&quot;: &quot;ACE1-PING&quot;, &quot;actions&quot;: { &quot;service-function-acl:rendered-service-path&quot;: rsp_name }, &quot;matches&quot;: { &quot;destination-ipv4-network&quot;: &quot;192.168.2.0/24&quot;, &quot;source-ipv4-network&quot;: &quot;192.168.2.0/24&quot;, &quot;protocol&quot;: &quot;1&quot; } } ] } },</p>
<pre><code>            {
                &quot;acl-name&quot;: &quot;ACL1-udp&quot;,
                &quot;acl-type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;,
                &quot;access-list-entries&quot;: {
                    &quot;ace&quot;: [
                        {
                            &quot;rule-name&quot;: &quot;ACE1-UDP&quot;,
                            &quot;actions&quot;: {
                                &quot;service-function-acl:rendered-service-path&quot;: rsp_name
                            },
                            &quot;matches&quot;: {
                                &quot;destination-ipv4-network&quot;: &quot;192.168.2.0/24&quot;,
                                &quot;source-ipv4-network&quot;: &quot;192.168.2.0/24&quot;,
                                &quot;protocol&quot;: &quot;17&quot;
                            }
                        }
                    ]
                }
            },

            {
                &quot;acl-name&quot;: &quot;ACL2-ping&quot;,
                &quot;acl-type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;,
                &quot;access-list-entries&quot;: {
                    &quot;ace&quot;: [
                        {
                            &quot;rule-name&quot;: &quot;ACE2-PING&quot;,
                            &quot;actions&quot;: {
                                &quot;service-function-acl:rendered-service-path&quot;: rsp_rev_name
                            },
                            &quot;matches&quot;: {
                                &quot;destination-ipv4-network&quot;: &quot;192.168.2.0/24&quot;,
                                &quot;source-ipv4-network&quot;: &quot;192.168.2.0/24&quot;,
                                &quot;protocol&quot;: &quot;1&quot;
                            }
                        }
                    ]
                }
            },

            {
                &quot;acl-name&quot;: &quot;ACL2-udp&quot;,
                &quot;acl-type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;,
                &quot;access-list-entries&quot;: {
                    &quot;ace&quot;: [
                        {
                            &quot;rule-name&quot;: &quot;ACE2-UDP&quot;,
                            &quot;actions&quot;: {
                                &quot;service-function-acl:rendered-service-path&quot;: rsp_rev_name
                            },
                            &quot;matches&quot;: {
                                &quot;destination-ipv4-network&quot;: &quot;192.168.2.0/24&quot;,
                                &quot;source-ipv4-network&quot;: &quot;192.168.2.0/24&quot;,
                                &quot;protocol&quot;: &quot;17&quot;
                            }
                        }
                    ]
                }
            },

            {
                &quot;acl-name&quot;: &quot;ACL2&quot;,
                &quot;acl-type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;,
                &quot;access-list-entries&quot;: {
                    &quot;ace&quot;: [
                        {
                            &quot;rule-name&quot;: &quot;ACE2&quot;,
                            &quot;actions&quot;: {
                                &quot;service-function-acl:rendered-service-path&quot;: rsp_rev_name
                            },
                            &quot;matches&quot;: {
                                &quot;destination-ipv4-network&quot;: &quot;192.168.2.0/24&quot;,
                                &quot;source-ipv4-network&quot;: &quot;192.168.2.0/24&quot;,
                                &quot;protocol&quot;: &quot;6&quot;
                            }
                        }
                    ]
                }
            }
        ]
    }
}</code></pre>
<p>def get_service_function_classifiers_uri(): return &quot;/restconf/config/service-function-classifier:service-function-classifiers/&quot;</p>
<p>def get_service_function_classifiers_data(): return { &quot;service-function-classifiers&quot;: { &quot;service-function-classifier&quot;: [ { &quot;name&quot;: &quot;Classifier1&quot;, &quot;scl-service-function-forwarder&quot;: [ { &quot;name&quot;: &quot;Classifier1&quot;, &quot;interface&quot;: &quot;veth-br&quot; } ], &quot;acl&quot;: { &quot;name&quot;: &quot;ACL1&quot;, &quot;type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot; } },</p>
<pre><code>            {
                &quot;name&quot;: &quot;Classifier1-udp&quot;,
                &quot;scl-service-function-forwarder&quot;: [
                    {
                        &quot;name&quot;: &quot;Classifier1&quot;,
                        &quot;interface&quot;: &quot;veth-br&quot;
                    }
                ],
                &quot;acl&quot;: {
                    &quot;name&quot;: &quot;ACL1-udp&quot;,
                    &quot;type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;
                }
            },


            {
                &quot;name&quot;: &quot;Classifier1-ping&quot;,
                &quot;scl-service-function-forwarder&quot;: [
                    {
                        &quot;name&quot;: &quot;Classifier1&quot;,
                        &quot;interface&quot;: &quot;veth-br&quot;
                    }
                ],
                &quot;acl&quot;: {
                    &quot;name&quot;: &quot;ACL1-ping&quot;,
                    &quot;type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;
                }
            },

            {
                &quot;name&quot;: &quot;Classifier2-ping&quot;,
                &quot;scl-service-function-forwarder&quot;: [
                    {
                        &quot;name&quot;: &quot;Classifier2&quot;,
                        &quot;interface&quot;: &quot;veth-br&quot;
                    }
                ],
                &quot;acl&quot;: {
                    &quot;name&quot;: &quot;ACL2-ping&quot;,
                    &quot;type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;
                }
            },

            {
                &quot;name&quot;: &quot;Classifier2-udp&quot;,
                &quot;scl-service-function-forwarder&quot;: [
                    {
                        &quot;name&quot;: &quot;Classifier2&quot;,
                        &quot;interface&quot;: &quot;veth-br&quot;
                    }
                ],
                &quot;acl&quot;: {
                    &quot;name&quot;: &quot;ACL2-udp&quot;,
                    &quot;type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;
                }
            },

            {
                &quot;name&quot;: &quot;Classifier2&quot;,
                &quot;scl-service-function-forwarder&quot;: [
                    {
                        &quot;name&quot;: &quot;Classifier2&quot;,
                        &quot;interface&quot;: &quot;veth-br&quot;
                    }
                ],
                &quot;acl&quot;: {
                    &quot;name&quot;: &quot;ACL2&quot;,
                    &quot;type&quot;: &quot;ietf-access-control-list:ipv4-acl&quot;
                }
            }
        ]
    }
}</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/10/11/k8s-tuto/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/10/11/k8s-tuto/" class="post-title-link" itemprop="url">k8s-tuto</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-11 15:44:48" itemprop="dateCreated datePublished" datetime="2019-10-11T15:44:48+08:00">2019-10-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-12-18 01:57:07" itemprop="dateModified" datetime="2019-12-18T01:57:07+08:00">2019-12-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/" itemprop="url" rel="index"><span itemprop="name">kubernetes</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kubernetes/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>K8S入门的阅读材料 第一阶段：对K8S有个大体宏观的介绍。推荐文章: https://www.servicemesher.com/blog/the-data-center-os-kubernetes/ 第二阶段：认真阅读 Kubernetes in actions. 由浅入深，循序渐进，条理清晰的好书 第三阶段：读一些使用案列，如 https://medium.com/better-programming/kubernetes-a-detailed-example-of-deployment-of-a-stateful-application-de3de33c8632 第五阶段： 方向1： 了解什么 service mesh 以及 Istio Istio本身就是很精妙的微服务系统，可以通过了解这个项目来深入学习 Kubenetes 方向2： 如果自己有实际需求就去实践</p>
<p>K8s有三层网络结构</p>
<p>Service网络 Pod 网络 Node 网络</p>
<p>真是的，物理的网络只有一个: Node 网络，其他网络均为virtual network</p>
<p>kubeadm k8s官方推荐的k8s集群安装方案 kubeadm 是运行在GCP平台 Harbor是个仓库</p>
<p>CentOS 7 (kernel version &gt;= 4.4.x)</p>
<p>Keep in mind that the app itself needs to support being scaled horizontally. Kubernetes doesn’t magically make your app scalable; it only makes it trivial to scale the app up or down.</p>
<figure>
<img src="/.com//k8s-app-outline.png" alt="The basic outline of an application running in Kubernetes"><span class="image-caption">The basic outline of an application running in Kubernetes</span><figcaption>The basic outline of an application running in Kubernetes</figcaption>
</figure>
<p>In k8s, apps run in form of pods.</p>
<ul>
<li>Pods are usually fronted by Service;</li>
<li>Pods are usually backened by ReplicationController/ReplicaSet;</li>
</ul>
<p>Pod和ReplicationController之间并没有绑定的关系。在任何时候，RepilicationController 管理和其label selector匹配的pod. 因此我们可以通过修改Pod的标签，来更改其和ReplicationController之间的关系。 ## 为啥么从一个pod的一个容器里去ping一个service 的IP 不通，但是curl却没有问题？ 我第一反应是防火墙设置。但是标准答案是：service’s cluster IP is a virtual IP, and only has meaning when combined with the service port.</p>
<figure>
<img src="/.com//daemon-set-vs-replica-set.png" alt="DaemonSets run only a single pod replica on each node, whereas ReplicaSets scatter them around the whole cluster randomly"><span class="image-caption">DaemonSets run only a single pod replica on each node, whereas ReplicaSets scatter them around the whole cluster randomly</span><figcaption>DaemonSets run only a single pod replica on each node, whereas ReplicaSets scatter them around the whole cluster randomly</figcaption>
</figure>
<h2 id="pod的生命周期管理工具有replicationcontrooler-replicaset-daemonset-job-说说这些工具之间的区别">Pod的生命周期管理工具有：ReplicationControoler, ReplicaSet, DaemonSet, Job 说说这些工具之间的区别。</h2>
<p>答案：</p>
<ul>
<li>ReplicationController 出现最早，按照label selector去选择Pod进行管理。它的任务是永远保证有通过replicas属性申明过的个数的Pod运行着。这些Pod一旦终止掉，马上再创建新的Pod替代</li>
<li>ReplicaSet 和 ReplicationController 功能基本一致，可被视为新一代 ReplicationController，并会最终取代ReplicationController。ReplicaSet 相较于ReplicationController, 表现力更强。如果想使用 ReplicaSet，在YAML文件中，apiVersion=apps/v1beta2</li>
<li>DaemontSet 是用来保证由label selector选出来的Pod在每个node上都有运行。DaemonSet 需要在YAML文件中声明 apiVersion=apps/v1beta2(注: 2019-10-23现在 DaemonSet 已经被正式添加到v1中了，反而apiVersion=apps/v1beta2会导致错误!)</li>
<li>Job 和其他三者比，管理的Pod是会运行完成的，而不是一致无限期运行下去的。Job 只保证在Pod执行的task完成之前，</li>
</ul>
<h2 id="请描述下-k8s-的设计原则以及总体框架">请描述下 k8s 的设计原则以及总体框架</h2>
<p>设计原则: portable, flexible, extensible, automatable 总体框架:</p>
<figure>
<img src="/.com//k8s-high-level-arch.png" alt="A High-Level Kubernetes Architecture"><span class="image-caption">A High-Level Kubernetes Architecture</span><figcaption>A High-Level Kubernetes Architecture<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></figcaption>
</figure>
<ul>
<li>Cluster state store: cluster state store is a persistent storage instance which stores the state of all the abstract Kubernetes objects configured in the system. The cluster state store has support for watch functionality. Through this functionality, all the coordinating components could be quickly notified whenever a change is made to an object. Kubernetes needs to store data somewhere. That’s what etcd is for! Kubenetes is a distributed system (i.e. master-worker mode) and thus requires a distributed database.</li>
<li>Controller Manager: this is the component of master that runs controllers. Controllers run loops and monitor the actual cluster state and state represented in the abstract Kubernetes objects.</li>
<li>Scheduler: It is the component of the master responsible for allocating physical resources on the cluster to run applications/jobs added to the abstract data store.</li>
</ul>
<h2 id="liveness-probe-和-readiness-probe-区别是啥">liveness probe 和 readiness probe 区别是啥？</h2>
<p>Kubernetes can check if a container is still alive through liveness probes. Kubenetes can probe a container using one of the three mechanisms:</p>
<ul>
<li>An HTTP GET probe</li>
<li>A TCP socket probe</li>
<li>An Exec probe</li>
</ul>
<p>The readiness probe is invoked periodically and determines whether the specific pod should receive client requests or not. The notion of being ready is obviously something that's specific to each container.</p>
<h2 id="问题为啥要用deployment">问题：为啥要用deployment?</h2>
<p>因为pod内容器有时候会有升级更新的需求。保持应用的可用性。不会导致在升级的过程中产品中断服务。滚动升级 滚动回退: 用含有新版本应用的镜像(在pod)里逐步替换用含有旧版本应用的镜像。比如一个含有新版本应用的镜像启动之后，一个含有旧版本的镜像才会被删除。</p>
<h2 id="问题鉴于每个pod实际上都有一个私有ip和主机名那么都有哪些方法可以访问到目标pod呢">问题：鉴于每个pod实际上都有一个私有IP和主机名，那么都有哪些方法可以访问到目标pod呢？</h2>
<p>答案：</p>
<ul>
<li>通过 service: <code>kubectl expose</code> 会为目标pod部署一个load balancer类型的service, 使得目标pod可以从外部同一个公有IP以及端口访问</li>
<li>通过 port forwarding: <code>kubectl port-forward</code> 可以为目标pod和本地主机之间做一个端口映射。具体实现有k8s给你做了。这种访问方式比较适合debugging以及一些其他目的。 <img src="/.com//k8s-port-forward-simplified-view.png" alt="A simplified view of what happens when you use curl with kubectl port-forward"><span class="image-caption">A simplified view of what happens when you use curl with kubectl port-forward</span></li>
</ul>
<h2 id="问-k8s-中-持久化存储都有哪些困难或者问题">问: k8s 中, 持久化存储都有哪些困难或者问题？</h2>
<ul>
<li>how to decouple the persistent storage volume create/mount and the infrastructure level storage implementation. so that dev. team has no need to deal with the actual storage technology used underneath =&gt; solution: PersistentVolume and PersistencVolumeClaim resource.</li>
<li>A cluster administrator need to provision the actual storage up in advance so that the users can decalre and use PV and PVC in their work. =&gt;solution: dynamic provisioning of PersistentVolumes. Cluster administrators deploy a PersistentVolume provisioner and define one or more StorageClass objects to let users choose what type of PersistentVolume they want.</li>
</ul>
<h2 id="k8s中你都接触过哪些类型的对象呢资源类型">k8s中你都接触过哪些类型的对象呢/资源类型？</h2>
<ul>
<li><p>pod: k8s可以管理调度的最小单元 each pod behaves like a separate independent machine with its own private IP address and hostname.</p></li>
<li><p>node:</p></li>
<li><p>ReplicationController: 当 k8s 集群中的master节点收到来自kubectl的，创建pod请求(即RESTFul API)的时候，master节点中会产生一个 ReplicationController对象(具体谁创建有待澄清)</p></li>
<li><p>Service: 如果需要使得一个pod可以从外部访问，那么我们需要通过 Service (具体说来是LoadBalancer类型)对象来暴露目标pod. 通过LoadBalancer类型的服务对象，一个外部负载均衡就会被创建，然后你可以通过这个负载均衡的公共地址来访问目标pod。</p></li>
<li>Endpoint: Service和Pod之间的关键对象</li>
<li><p>注意：kubernetes in action 成书较早，该书中示例使用的 kubectl run --generator=run/v1 已经不再推荐使用，未来会被 kubectl run --generator=run-pod/v1或者kubectl create替代。</p></li>
<li><p>PersistentVolumes and PersistentVolumeClaims: to enable apps to request storage in a Kubernetes cluster without having to deal with infrastructure specifics, PC and PVC are introduced. 这两个名字起得很有误导性，因为诸如hostPath之类的存储也可以实现数据持久化。下面这个图可以很好地解释 PersistentVolume, PersistentVolumeClaim，以及 Infrastructure-level storage 实现之间的关系 <img src="/.com//k8s-pv-pvc-illustration.png" alt="PersistentVolumes are provisioned by cluster admins and consumed by pods through PersistentVolumeClaims"><span class="image-caption">PersistentVolumes are provisioned by cluster admins and consumed by pods through PersistentVolumeClaims</span></p></li>
</ul>
<figure>
<img src="/.com//k8s-pv-pvc-namespace.png" alt="PersistentVolumes, like cluster Nodes, don’t belong to any namespace, unlike pods and PersistentVolumeClaims"><span class="image-caption">PersistentVolumes, like cluster Nodes, don’t belong to any namespace, unlike pods and PersistentVolumeClaims</span><figcaption>PersistentVolumes, like cluster Nodes, don’t belong to any namespace, unlike pods and PersistentVolumeClaims</figcaption>
</figure>
<figure>
<img src="/.com//k8s-pv-dynamic-provision.png" alt="The complete picture of dynamic provisioning of PersistentVolumes"><span class="image-caption">The complete picture of dynamic provisioning of PersistentVolumes</span><figcaption>The complete picture of dynamic provisioning of PersistentVolumes</figcaption>
</figure>
<p>Referece</p>
<p>for i in 1 2 3 4 5 do sleep 5;docker exec -it sf1 pidstat -p 11 1 5 | grep &quot;Average&quot; done</p>
<section class="footnotes">
<hr>
<ol>
<li id="fn1"><p>https://medium.com/better-programming/kubernetes-a-detailed-example-of-deployment-of-a-stateful-application-de3de33c8632<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/09/06/docker-overlay-net-deep-tuto-zn3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/06/docker-overlay-net-deep-tuto-zn3/" class="post-title-link" itemprop="url">深入了解 Docker overlay 网络机制3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-06 21:40:45" itemprop="dateCreated datePublished" datetime="2019-09-06T21:40:45+08:00">2019-09-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-09-07 19:49:04" itemprop="dateModified" datetime="2019-09-07T19:49:04+08:00">2019-09-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="introduction">Introduction</h1>
<p>In part 1 of this blog post we have seen how Docker creates a dedicated namespace for the overlay and connect the containers to this namespace. In part 2 we have looked in details at how Docker uses VXLAN to tunnel traffic between the hosts in the overlay. In this third post, we will see how we can create our own overlay with standard Linux commands.</p>
<h1 id="manual-overlay-creation">Manual overlay creation</h1>
<p>If you have tried the commands from the first two posts, you need to clean-up your Docker hosts by removing all our containers and the overlay network: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker rm -f $(docker ps -aq)</span><br><span class="line">docker0:~$ docker network rm demonet</span><br><span class="line">docker1:~$ docker rm -f $(docker ps -aq)</span><br></pre></td></tr></table></figure> The first thing we are going to do now is to create an network namespace called “overns”: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip netns add overns</span><br></pre></td></tr></table></figure> Now we are going to create a bridge in this namespace, give it an IP address and bring the interface up: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip netns exec overns ip link add dev br0 type bridge</span><br><span class="line">docker0:~$ sudo ip netns exec overns ip addr add dev br0 192.168.0.1&#x2F;24</span><br><span class="line">docker0:~$ sudo ip netns exec overns ip link set br0 up</span><br></pre></td></tr></table></figure></p>
<p>The next step is to create a VXLAN interface and attach it to the bridge: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip link add dev vxlan1 type vxlan id 42 proxy learning dstport 4789</span><br><span class="line">docker0:~$ sudo ip link set vxlan1 netns overns</span><br><span class="line">docker0:~$ sudo ip netns exec overns ip link set vxlan1 master br0</span><br><span class="line">docker0:~$ sudo ip netns exec overns ip link set vxlan1 up</span><br></pre></td></tr></table></figure></p>
<p>The most important command so far is the creation of the VxLAN interface. We configured it to use VxLAN id 42 and to tunnel traffic on the standard VxLAN port. The proxy option allows the vxlan interface to answer ARP queries (we have seen it in part 2). We will discuss the <code>learning</code> option later in this post. Notice that we did not create the VxLAN interface inside the <code>overns</code>namespace but on the host and then moved it to <code>overns</code> namespace. This is necessary so the VxLAN interface can keep a link with our main host interface and send traffic over the network. If we had created the interface inside the <code>overns</code> namespace (like we did for <code>br0</code>) we would not have been able to send traffic outside the <code>overns</code> namespace.</p>
<p>Once we have run these commands on both <code>docker0</code> and <code>docker1</code>, here is what we have: <img src="/.com//overlay-1.png" alt="VXLAN interface and bridge in an overlay namespace"><span class="image-caption">VXLAN interface and bridge in an overlay namespace</span></p>
<p>Now we will create containers and connect them to our bridge. Let’s start with docker0. First, we create a container: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker run -d --net&#x3D;none --name&#x3D;demo debian sleep 3600</span><br></pre></td></tr></table></figure></p>
<p>We will need the path of the network namespace for this container. We can find it by inspecting the container. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ ctn_ns_path&#x3D;$(docker inspect --format&#x3D;&quot;&#123;&#123; .NetworkSettings.SandboxKey&#125;&#125;&quot; demo)</span><br></pre></td></tr></table></figure></p>
<p>Our container has no network connectivity because of the <code>--net=none</code> option. We now create a <code>veth</code> device and move one of its endpoints (<code>veth1</code>) to our overlay network namespace (i.e. <code>overns</code>), attach it to the bridge and bring it up.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip link add dev veth1 mtu 1450 type veth peer name veth2 mtu 1450</span><br><span class="line">docker0:~$ sudo ip link set dev veth1 netns overns</span><br><span class="line">docker0:~$ sudo ip netns exec overns ip link set veth1 master br0</span><br><span class="line">docker0:~$ sudo ip netns exec overns ip link set veth1 up</span><br></pre></td></tr></table></figure>
<p>The first command uses an MTU of 1450 which is necessary due to the overhead added by the VxLAN header.</p>
<p>The last step is to configure veth2: send it to our container network namespace and configure it with a MAC address (02:42:c0:a8:00:02) and an IP address (192.168.0.2): <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ ctn_ns&#x3D;$&#123;ctn_ns_path##*&#x2F;&#125;</span><br><span class="line">docker0:~$ sudo ln -sf $ctn_ns_path &#x2F;var&#x2F;run&#x2F;netns&#x2F;$ctn_ns</span><br><span class="line">docker0:~$ sudo ip link set dev veth2 netns $ctn_ns</span><br><span class="line"></span><br><span class="line">docker0:~$ sudo ip netns exec $ctn_ns ip link set dev veth2 name eth0 address 02:42:c0:a8:00:02</span><br><span class="line">docker0:~$ sudo ip netns exec $ctn_ns ip addr add dev eth0 192.168.0.2&#x2F;24</span><br><span class="line">docker0:~$ sudo ip netns exec $ctn_ns ip link set dev eth0 up</span><br><span class="line"></span><br><span class="line">docker0:~$ sudo rm &#x2F;var&#x2F;run&#x2F;netns&#x2F;$ctn_ns</span><br></pre></td></tr></table></figure></p>
<p>The symbolic link in <code>/var/run/netns</code> is required so we can use the native <code>ip netns</code> commands (to move the interface to the container network namespace). We used the same addressing schem as Docker: the last 4 bytes of the MAC address match the IP address of the container and the second one is the VxLAN id.</p>
<p>We have to do the same on <code>docker1</code> with different MAC and IP addresses (02:42:c0:a8:00:03 and 192.168.0.3). If you use the <code>terraform</code> stack from the <a href="https://github.com/lbernail/dockercon2017" target="_blank" rel="noopener">github repository</a>, there is a helper shell script to attach the container to the overlay. We can use it on docker1: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ docker run -d --net&#x3D;none --name&#x3D;demo debian sleep 3600</span><br><span class="line">docker1:~$ .&#x2F;attach-ctn.sh demo 3</span><br></pre></td></tr></table></figure></p>
<p>The first parameter is the name of the container to attach and the second one is the final digit of the MAC/IP addresses. Here is the setup we have gotten: <img src="/.com//overlay-2.png" alt="Connecting containers to our overlay"><span class="image-caption">Connecting containers to our overlay</span></p>
<p>Now that our containers are configured, we can test connectivity: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec -it demo ping 192.168.0.3</span><br><span class="line">PING 192.168.0.3 (192.168.0.3): 56 data bytes</span><br><span class="line">92 bytes from 192.168.0.2: Destination Host Unreachable</span><br></pre></td></tr></table></figure></p>
<p>We are not able to ping yet. Let’s try to understand why by looking at the ARP entries in the container and in the overlay namespace: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec demo ip neighbor show</span><br><span class="line">docker0:~$ sudo ip netns exec overns ip neighbor show</span><br></pre></td></tr></table></figure></p>
<p>Both commands do not return any result: they do not know what is the MAC address associated with IP 192.168.0.3. We can verify that our command is generating an ARP query by running tcpdump in the overlay namespace: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip netns exec overns tcpdump -i br0</span><br><span class="line">docker0:~$ tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br></pre></td></tr></table></figure></p>
<p>If we rerun the ping command from another terminal, here is the tcpdump output we get: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">17:15:27.074500 ARP, Request who-has 192.168.0.3 tell 192.168.0.2, length 28</span><br><span class="line">17:15:28.071265 ARP, Request who-has 192.168.0.3 tell 192.168.0.2, length 28</span><br></pre></td></tr></table></figure></p>
<p>The ARP query is broadcasted and received by our overlay namespace but does not receive any answer. We have seen in part 2 that the Docker daemon populates the ARP and FDB tables and makes use of the <code>proxy</code> option of the VxLAN interface to answer these queries. We configured our interface with this option so we can do the same by simply populating the ARP and FDB entries in the overlay namespace: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip netns exec overns ip neighbor add 192.168.0.3 lladdr 02:42:c0:a8:00:03 dev vxlan1</span><br><span class="line">docker0:~$ sudo ip netns exec overns bridge fdb add 02:42:c0:a8:00:03 dev vxlan1 self dst 10.0.0.11 vni 42 port 4789</span><br></pre></td></tr></table></figure></p>
<p>The first command creates the ARP entry for 192.168.0.3 and the second one configures the forwarding table by telling it the MAC address is accessible using the VxLAN interface, with VxLAN id 42 and on host 10.0.0.11.</p>
<p>Do we have connectivity? <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec -it demo ping 192.168.0.3</span><br><span class="line">PING 192.168.0.3 (192.168.0.3): 56 data bytes</span><br><span class="line">^C--- 192.168.0.3 ping statistics ---</span><br><span class="line">3 packets transmitted, 0 packets received, 100% packet loss</span><br></pre></td></tr></table></figure></p>
<p>No yet, which makes sense because we have not configured docker1: the ICMP request is received by the container on docker1 but it does not know how to answer. We can verify this on docker1: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ sudo ip netns exec overns ip neighbor show</span><br><span class="line"></span><br><span class="line">docker1:~$ sudo ip netns exec overns bridge fdb show</span><br><span class="line">0e:70:32:15:1d:01 dev vxlan1 vlan 0 master br0 permanent</span><br><span class="line">02:42:c0:a8:00:03 dev veth1 vlan 0 master br0</span><br><span class="line">ca:9c:c1:c7:16:f2 dev veth1 vlan 0 master br0 permanent</span><br><span class="line">02:42:c0:a8:00:02 dev vxlan1 vlan 0 master br0</span><br><span class="line">02:42:c0:a8:00:02 dev vxlan1 dst 10.0.0.10 self</span><br><span class="line">33:33:00:00:00:01 dev veth1 self permanent</span><br><span class="line">01:00:5e:00:00:01 dev veth1 self permanent</span><br><span class="line">33:33:ff:c7:16:f2 dev veth1 self permanent</span><br></pre></td></tr></table></figure></p>
<p>The first command shows, as expected, that we do not have any ARP information about <code>192.168.0.2</code>. The output of the second command is more surprising because we can see the entry in the forwarding database for our container on docker0. What happened is the following: when the ICMP request reached the interface, the entry was “learned” and added to the database. This behavior is made possible by the <code>learning</code> option of the VxLAN interface. Let’s add the ARP information on docker1 and verify that we can now ping: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ sudo ip netns exec overns ip neighbor add 192.168.0.2 lladdr 02:42:c0:a8:00:02 dev vxlan1</span><br><span class="line"></span><br><span class="line">docker0:~$ docker exec -it demo ping 192.168.0.3</span><br><span class="line">PING 192.168.0.3 (192.168.0.3): 56 data bytes</span><br><span class="line">64 bytes from 192.168.0.3: icmp_seq&#x3D;0 ttl&#x3D;64 time&#x3D;1.737 ms</span><br><span class="line">64 bytes from 192.168.0.3: icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.494 ms</span><br></pre></td></tr></table></figure></p>
<p>We have successfuly built an overlay with standard Linux commands: <img src="/.com//overlay-3.png" alt="Overview of our manual overlay"><span class="image-caption">Overview of our manual overlay</span></p>
<h1 id="dynamic-container-discovery">Dynamic container discovery</h1>
<p>We have just created an overlay from scratch. However, we need to manually create ARP and FDB entries for containers to talk to each other. We will now look at how this discovery process can be automated.</p>
<p>Let us first clean up our setup to start from scratch: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker rm -f $(docker ps -aq)</span><br><span class="line">docker0:~$ sudo ip netns delete overns</span><br><span class="line">docker1:~$ docker rm -f $(docker ps -aq)</span><br><span class="line">docker1:~$ sudo ip netns delete overns</span><br></pre></td></tr></table></figure></p>
<h1 id="catching-network-events-netlink">Catching network events: NETLINK</h1>
<p><a href="https://en.wikipedia.org/wiki/Netlink" target="_blank" rel="noopener"><code>Netlink</code></a> is used to transfer information between the kernel and user-space processes. <code>iproute2</code>(refer to this <a href="https://www.digitalocean.com/community/tutorials/how-to-use-iproute2-tools-to-manage-network-configuration-on-a-linux-vps" target="_blank" rel="noopener">tutorial</a> for more details), which we used earlier to configure interfaces, relies on <code>Netlink</code> to get/send configuration information to the kernel. It consists of multiple protocols ''families'' to communicate with different kernel components. The most common protocol is <code>NETLINK_ROUTE</code> which is the interface for routing and link configuration.</p>
<p>For each protocol, <code>Netlink</code> messages are organized by groups, for example for <code>NETLINK_ROUTE</code> you have:</p>
<ul>
<li>RTMGRP_LINK: link related messages</li>
<li>RTMGRP_NEIGH: neighbor related messages</li>
<li>many others</li>
</ul>
<p>For each group, you then have multiple notifications, for example:</p>
<ul>
<li>RTMGRP_LINK:
<ul>
<li>RTM_NEWLINK: A link was created</li>
<li>RTM_DELLINK: A link was deleted</li>
</ul></li>
<li>RTMGRP_NEIGH:
<ul>
<li>RTM_NEWNEIGH: A neighbor was added</li>
<li>RTM_DELNEIGH: A neighbor was deleted</li>
<li>RTM_GETNEIGH: The kernel is looking for a neighbor</li>
</ul></li>
</ul>
<p>I described the messages received in userspace when the kernel is sending notifications for these events, but similar messages can be sent to the kernel to configure links or neighbors.</p>
<p><code>iproute2</code> allows us to listen to <code>Netlink</code> events using the <code>monitor</code> subcommand. If we want to monitor for link information for instance: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ ip monitor link</span><br></pre></td></tr></table></figure></p>
<p>In another terminal on <code>docker0</code>, we can create a link and then delete it: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip link add dev veth1 type veth peer name veth2</span><br><span class="line">docker0:~$ sudo ip link del veth1</span><br></pre></td></tr></table></figure></p>
<p>On the first terminal we can see some output. When we created the interfaces: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">32: veth2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default</span><br><span class="line">    link&#x2F;ether b6:95:d6:b4:21:e9 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">33: veth1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default</span><br><span class="line">    link&#x2F;ether a6:e0:7a:da:a9:ea brd ff:ff:ff:ff:ff:ff</span><br></pre></td></tr></table></figure></p>
<p>When we removed them: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Deleted 33: veth1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default</span><br><span class="line">    link&#x2F;ether a6:e0:7a:da:a9:ea brd ff:ff:ff:ff:ff:ff</span><br><span class="line">Deleted 32: veth2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default</span><br><span class="line">    link&#x2F;ether b6:95:d6:b4:21:e9 brd ff:ff:ff:ff:ff:ff</span><br></pre></td></tr></table></figure> We can use this command to monitor other events: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ ip monitor route</span><br></pre></td></tr></table></figure> In another terminal: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip route add 8.8.8.8 via 10.0.0.1</span><br><span class="line">docker0:~$ sudo ip route del 8.8.8.8 via 10.0.0.1</span><br></pre></td></tr></table></figure> We get the following output: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">8.8.8.8 via 10.0.0.1 dev eth0</span><br><span class="line">Deleted 8.8.8.8 via 10.0.0.1 dev eth0</span><br></pre></td></tr></table></figure></p>
<p>In our case we are interested in neighbor events, in particular for <code>RTM_GETNEIGH</code> which are generated when the kernel does not have neighbor information and sends this notification to userspace so an application can create it. By default, this event is not sent to userspace but we can enable it and monitor neighbor notifications: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ echo 1 | sudo tee -a &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;neigh&#x2F;eth0&#x2F;app_solicit</span><br><span class="line">docker0:~$ ip monitor neigh</span><br></pre></td></tr></table></figure> This setting will not be necessary afterwards because the <code>l2miss</code> and <code>l3miss</code> options of our VxLAN interface will generate the <code>RTM_GETNEIGH</code> events.</p>
<p>In a second terminal, we can now trigger the generation of the GETNEIGH event: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ ping 10.0.0.100</span><br></pre></td></tr></table></figure> Here is the output we get: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.0.0.100 dev eth0  FAILED</span><br><span class="line">miss 10.0.0.100 dev eth0  INCOMPLETE</span><br></pre></td></tr></table></figure> We can use the same command in containers attached to our overlay. Let’s create an overlay and attach a container to it. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ .&#x2F;create-overlay.sh</span><br><span class="line">docker0:~$ docker run -d --net&#x3D;none --name&#x3D;demo debian sleep 3600</span><br><span class="line">docker0:~$ .&#x2F;attach-ctn.sh demo 2</span><br><span class="line">docker0:~$ docker exec demo ip monitor neigh</span><br></pre></td></tr></table></figure> The two shell scripts are available on the github <a href="https://github.com/lbernail/dockercon2017" target="_blank" rel="noopener">repo</a>.</p>
<p><code>create-overlay</code> creates an overlay called <code>overns</code> using the commands presented earlier: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">sudo ip netns delete overns 2&gt; &#x2F;dev&#x2F;null &amp;&amp; echo &quot;Deleting existing overlay&quot;</span><br><span class="line">sudo ip netns add overns</span><br><span class="line">sudo ip netns exec overns ip link add dev br0 type bridge</span><br><span class="line">sudo ip netns exec overns ip addr add dev br0 192.168.0.1&#x2F;24</span><br><span class="line"></span><br><span class="line">sudo ip link add dev vxlan1 type vxlan id 42 proxy learning l2miss l3miss dstport 4789</span><br><span class="line">sudo ip link set vxlan1 netns overns</span><br><span class="line">sudo ip netns exec overns ip link set vxlan1 master br0</span><br><span class="line"></span><br><span class="line">sudo ip netns exec overns ip link set vxlan1 up</span><br><span class="line">sudo ip netns exec overns ip link set br0 up</span><br></pre></td></tr></table></figure></p>
<p><code>attach-ctn</code> attaches a container to the overlay. The first parameter is the name of the container and the second one the last byte of its IP address: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">ctn&#x3D;$&#123;1:-demo&#125;</span><br><span class="line">ip&#x3D;$&#123;2:-2&#125;</span><br><span class="line"></span><br><span class="line">ctn_ns_path&#x3D;$(docker inspect --format&#x3D;&quot;&#123;&#123; .NetworkSettings.SandboxKey&#125;&#125;&quot; $ctn)</span><br><span class="line">ctn_ns&#x3D;$&#123;ctn_ns_path##*&#x2F;&#125;</span><br><span class="line"></span><br><span class="line"># create veth interfaces</span><br><span class="line">sudo ip link add dev veth1 mtu 1450 type veth peer name veth2 mtu 1450</span><br><span class="line"></span><br><span class="line"># attach first peer to the bridge in our overlay namespace</span><br><span class="line">sudo ip link set dev veth1 netns overns</span><br><span class="line">sudo ip netns exec overns ip link set veth1 master br0</span><br><span class="line">sudo ip netns exec overns ip link set veth1 up</span><br><span class="line"></span><br><span class="line"># crate symlink to be able to use ip netns commands</span><br><span class="line">sudo ln -sf $ctn_ns_path &#x2F;var&#x2F;run&#x2F;netns&#x2F;$ctn_ns</span><br><span class="line">sudo ip link set dev veth2 netns $ctn_ns</span><br><span class="line"></span><br><span class="line"># move second peer tp container network namespace and configure it</span><br><span class="line">sudo ip netns exec $ctn_ns ip link set dev veth2 name eth0 address 02:42:c0:a8:00:0$&#123;ip&#125;</span><br><span class="line">sudo ip netns exec $ctn_ns ip addr add dev eth0 192.168.0.$&#123;ip&#125;&#x2F;24</span><br><span class="line">sudo ip netns exec $ctn_ns ip link set dev eth0 up</span><br><span class="line"></span><br><span class="line"># Clean up symlink</span><br><span class="line">sudo rm &#x2F;var&#x2F;run&#x2F;netns&#x2F;$</span><br></pre></td></tr></table></figure> We can now run <code>ip monitor</code> in the container: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec demo ip monitor neigh</span><br></pre></td></tr></table></figure> In a second terminal, we can ping an unknown host to generate GETNEIGH events: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec demo ping 192.168.0.3</span><br></pre></td></tr></table></figure> In the first terminal we can see the neighbor events: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.0.3 dev eth0  FAILED</span><br></pre></td></tr></table></figure> We can also look in the network namespace of the overlay: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip netns exec overns ip monitor neigh</span><br><span class="line">miss 192.168.0.3 dev vxlan1  STALE</span><br></pre></td></tr></table></figure></p>
<p>This event is slightly different because it is generated by the vxlan interface (because we created the interface with the <code>l2miss</code> and <code>l3miss</code> options). Let’s add the neighbor entry to the overlay namespace: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip netns exec overns ip neighbor add 192.168.0.3 lladdr 02:42:c0:a8:00:03 dev vxlan1 nud permanent</span><br></pre></td></tr></table></figure></p>
<p>If we run the <code>ip monitor neigh</code> command and try to ping from the other terminal, here is what we get: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip netns exec overns ip monitor neigh</span><br><span class="line">miss dev vxlan1 lladdr 02:42:c0:a8:00:03 STALE</span><br></pre></td></tr></table></figure></p>
<p>Now that we have the ARP information, we are getting an <code>l2miss</code> because we do not know where the MAC address is located in the overlay. Let’s add this information: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip netns exec overns bridge fdb add 02:42:c0:a8:00:03 dev vxlan1 self dst 10.0.0.11 vni 42 port 4789</span><br></pre></td></tr></table></figure></p>
<p>If we run the <code>ip monitor neigh</code> command again and try to ping we will not see neighbor events anymore.</p>
<p>The ip monitor command is very useful to see what is happening but in our case we want to catch these events to populate L2 and L3 information so we need to interact with them programmatically.</p>
<p>Here is simple python to subscribe to Netlink messages and decode GETNEIGH events: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"></span><br><span class="line"># Create the netlink socket and bind to NEIGHBOR NOTIFICATION,</span><br><span class="line">s &#x3D; socket.socket(socket.AF_NETLINK, socket.SOCK_RAW, socket.NETLINK_ROUTE)</span><br><span class="line">s.bind((os.getpid(), RTMGRP_NEIGH))</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    data &#x3D; s.recv(65535)</span><br><span class="line">    msg_len, msg_type, flags, seq, pid &#x3D; struct.unpack(&quot;&#x3D;LHHLL&quot;, data[:16])</span><br><span class="line"></span><br><span class="line">    # We fundamentally only care about GETNEIGH messages</span><br><span class="line">    if msg_type !&#x3D; RTM_GETNEIGH:</span><br><span class="line">        continue</span><br><span class="line"></span><br><span class="line">    data&#x3D;data[16:]</span><br><span class="line">    ndm_family, _, _, ndm_ifindex, ndm_state, ndm_flags, ndm_type &#x3D; struct.unpack(&quot;&#x3D;BBHiHBB&quot;, data[:12])</span><br><span class="line">    logging.debug(&quot;Received a Neighbor miss&quot;)</span><br><span class="line">    logging.debug(&quot;Family: &#123;&#125;&quot;.format(if_family.get(ndm_family,ndm_family)))</span><br><span class="line">    logging.debug(&quot;Interface index: &#123;&#125;&quot;.format(ndm_ifindex))</span><br><span class="line">    logging.debug(&quot;State: &#123;&#125;&quot;.format(nud_state.get(ndm_state,ndm_state)))</span><br><span class="line">    logging.debug(&quot;Flags: &#123;&#125;&quot;.format(ndm_flags))</span><br><span class="line">    logging.debug(&quot;Type: &#123;&#125;&quot;.format(type.get(ndm_type,ndm_type)))</span><br><span class="line"></span><br><span class="line">    data&#x3D;data[12:]</span><br><span class="line">    rta_len, rta_type &#x3D; struct.unpack(&quot;&#x3D;HH&quot;, data[:4])</span><br><span class="line">    logging.debug(&quot;RT Attributes: Len: &#123;&#125;, Type: &#123;&#125;&quot;.format(rta_len,nda_type.get(rta_type,rta_type)))</span><br><span class="line"></span><br><span class="line">    data&#x3D;data[4:]</span><br><span class="line">    if nda_type.get(rta_type,rta_type) &#x3D;&#x3D; &quot;NDA_DST&quot;:</span><br><span class="line">      dst&#x3D;socket.inet_ntoa(data[:4])</span><br><span class="line">      logging.info(&quot;L3Miss: Who has IP: &#123;&#125;?&quot;.format(dst))</span><br><span class="line"></span><br><span class="line">    if nda_type.get(rta_type,rta_type) &#x3D;&#x3D; &quot;NDA_LLADDR&quot;:</span><br><span class="line">      mac&#x3D;&quot;%02x:%02x:%02x:%02x:%02x:%02x&quot; % struct.unpack(&quot;BBBBBB&quot;,data[:6])</span><br><span class="line">      logging.info(&quot;L2Miss: Who has MAC: &#123;&#125;?&quot;.format(mac))</span><br></pre></td></tr></table></figure></p>
<p>This script only contains the interesting lines, the full one is available on the github repository. Let’s go quickly through the most important part of the script. First, we create the NETLINK socket, configure it for NETLINK_ROUTE protocol and subscribe to the neighbor event group (RTMGRP_NEIGH): <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s &#x3D; socket.socket(socket.AF_NETLINK, socket.SOCK_RAW, socket.NETLINK_ROUTE)</span><br><span class="line">s.bind((os.getpid(), RTMGRP_NEIGH))</span><br></pre></td></tr></table></figure></p>
<p>The we decode the message and filter to only process GETNEIGH messages: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">msg_len, msg_type, flags, seq, pid &#x3D; struct.unpack(&quot;&#x3D;LHHLL&quot;, data[:16])</span><br><span class="line"># We fundamentally only care about GETNEIGH messages</span><br><span class="line">if msg_type !&#x3D; RTM_GETNEIGH:</span><br><span class="line">    continue</span><br></pre></td></tr></table></figure></p>
<p>To understand how the message is decoded, here is a representation of the message. The Netlink header is represented in orange: Once we have a <code>GETNEIGH</code> message we can decode the ndmsg header (in blue): <img src="/.com//getneigh_message.png" alt="GETNEIGH Netlink message structure"><span class="image-caption">GETNEIGH Netlink message structure</span> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ndm_family, _, _, ndm_ifindex, ndm_state, ndm_flags, ndm_type &#x3D; struct.unpack(&quot;&#x3D;BBHiHBB&quot;, data[:12])</span><br></pre></td></tr></table></figure></p>
<p>This header is followed by an <code>rtattr</code> structure, which contains the data we are interested in. First we decode the header of the structure (purple): <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rta_len, rta_type &#x3D; struct.unpack(&quot;&#x3D;HH&quot;, data[:4])</span><br></pre></td></tr></table></figure></p>
<p>We can receive two different types of messages:</p>
<ul>
<li>NDA_DST: L3 miss, the kernel is looking for the MAC address associated with the IP in the data field (4 data bytes after the rta header)</li>
<li>NDA_LLADDR: L2 miss, the kernel is looking for the vxlan host for the MAC address in the data field (6 data bytes after the rta header) <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data&#x3D;data[4:]</span><br><span class="line">if nda_type.get(rta_type,rta_type) &#x3D;&#x3D; &quot;NDA_DST&quot;:</span><br><span class="line">  dst&#x3D;socket.inet_ntoa(data[:4])</span><br><span class="line">  logging.info(&quot;L3Miss: Who has IP: &#123;&#125;?&quot;.format(dst))</span><br><span class="line"></span><br><span class="line">if nda_type.get(rta_type,rta_type) &#x3D;&#x3D; &quot;NDA_LLADDR&quot;:</span><br><span class="line">  mac&#x3D;&quot;%02x:%02x:%02x:%02x:%02x:%02x&quot; % struct.unpack(&quot;BBBBBB&quot;,data[:6])</span><br><span class="line">  logging.info(&quot;L2Miss: Who has MAC: &#123;&#125;?&quot;.format(mac))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>We can try this script in our overlay (we recreate everything to start with a clean environment): <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker rm -f $(docker ps -aq)</span><br><span class="line">docker0:~$ .&#x2F;create-overlay.sh</span><br><span class="line">docker0:~$ docker run -d --net&#x3D;none --name&#x3D;demo debian sleep 3600</span><br><span class="line">docker0:~$ .&#x2F;attach-ctn.sh demo 2</span><br><span class="line">docker0:~$ sudo ip netns exec overns python&#x2F;l2l3miss.py</span><br></pre></td></tr></table></figure></p>
<p>If we try to ping from another terminal: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec -it demo ping 192.168.0.3</span><br></pre></td></tr></table></figure></p>
<p>Here is the output we get: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INFO:root:L3Miss: Who has IP: 192.168.0.3?</span><br></pre></td></tr></table></figure></p>
<p>If we add the neighbor information and ping again: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ip netns exec overns ip neighbor add 192.168.0.3 lladdr 02:42:c0:a8:00:03 dev vxlan1</span><br><span class="line">docker0:~$ docker exec -it demo ping 192.168.0.3</span><br></pre></td></tr></table></figure></p>
<p>We now get an L2 miss because we have added the L3 information. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INFO:root:L2Miss: Who has MAC: 02:42:c0:a8:00:03?</span><br></pre></td></tr></table></figure></p>
<h1 id="dynamic-discovery-with-consul">Dynamic discovery with Consul</h1>
<p>Now that we have seen how we can be notified of L2 and L3 misses and catch these events in python, we will store all L2 and L3 data in Consul and add the entries in the overlay namespace when we get a neighbor event.</p>
<p>First, we are going to create the entries in Consul. We can do this using the web interface or curl: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:$ curl -X PUT -d &#39;02:42:c0:a8:00:02&#39; http:&#x2F;&#x2F;consul:8500&#x2F;v1&#x2F;kv&#x2F;demo&#x2F;arp&#x2F;192.168.0.2</span><br><span class="line">docker0:$ curl -X PUT -d &#39;02:42:c0:a8:00:03&#39; http:&#x2F;&#x2F;consul:8500&#x2F;v1&#x2F;kv&#x2F;demo&#x2F;arp&#x2F;192.168.0.3</span><br><span class="line">docker0:$ curl -X PUT -d &#39;10.0.0.10&#39; http:&#x2F;&#x2F;consul:8500&#x2F;v1&#x2F;kv&#x2F;demo&#x2F;fib&#x2F;02:42:c0:a8:00:02</span><br><span class="line">docker0:$ curl -X PUT -d &#39;10.0.0.11&#39; http:&#x2F;&#x2F;consul:8500&#x2F;v1&#x2F;kv&#x2F;demo&#x2F;fib&#x2F;02:42:c0:a8:00:03</span><br></pre></td></tr></table></figure> We create two types of entries:</p>
<ul>
<li>ARP: using the keys <code>demo/arp/{IP address}</code> with the MAC address as the value</li>
<li>FIB: using the keys <code>demo/arp/{MAC address}</code> with the IP address of the server in the overlay hosting this Mac address</li>
</ul>
<p>In the web interface, we get this for ARP keys: <img src="/.com//consul_arp.png" alt="Consul ARP entries"><span class="image-caption">Consul ARP entries</span></p>
<p>Now we just need to look up data when we receive a <code>GETNEIGH</code> event and populate the ARP or FIB tables using Consul data. Here is a (slightly simplified) python script which does this: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">from pyroute2 import NetNS</span><br><span class="line"></span><br><span class="line">vxlan_ns&#x3D;&quot;overns&quot;</span><br><span class="line">consul_host&#x3D;&quot;consul&quot;</span><br><span class="line">consul_prefix&#x3D;&quot;demo&quot;</span><br><span class="line"></span><br><span class="line">ipr &#x3D; NetNS(vxlan_ns)</span><br><span class="line">ipr.bind()</span><br><span class="line"></span><br><span class="line">c&#x3D;consul.Consul(host&#x3D;consul_host,port&#x3D;8500)</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">  msg&#x3D;ipr.get()</span><br><span class="line">  for m in msg:</span><br><span class="line">    if m[&#39;event&#39;] !&#x3D; &#39;RTM_GETNEIGH&#39;:</span><br><span class="line">      continue</span><br><span class="line"></span><br><span class="line">    ifindex&#x3D;m[&#39;ifindex&#39;]</span><br><span class="line">    ifname&#x3D;ipr.get_links(ifindex)[0].get_attr(&quot;IFLA_IFNAME&quot;)</span><br><span class="line"></span><br><span class="line">    if m.get_attr(&quot;NDA_DST&quot;) is not None:</span><br><span class="line">      ipaddr&#x3D;m.get_attr(&quot;NDA_DST&quot;)</span><br><span class="line">      logging.info(&quot;L3Miss on &#123;&#125;: Who has IP: &#123;&#125;?&quot;.format(ifname,ipaddr))</span><br><span class="line"></span><br><span class="line">      (idx,answer)&#x3D;c.kv.get(consul_prefix+&quot;&#x2F;arp&#x2F;&quot;+ipaddr)</span><br><span class="line">      if answer is not None:</span><br><span class="line">        mac_addr&#x3D;answer[&quot;Value&quot;]</span><br><span class="line">        logging.info(&quot;Populating ARP table from Consul: IP &#123;&#125; is &#123;&#125;&quot;.format(ipaddr,mac_addr))</span><br><span class="line">        try:</span><br><span class="line">            ipr.neigh(&#39;add&#39;, dst&#x3D;ipaddr, lladdr&#x3D;mac_addr, ifindex&#x3D;ifindex, state&#x3D;ndmsg.states[&#39;permanent&#39;])</span><br><span class="line">        except NetlinkError as (code,message):</span><br><span class="line">            print(message)</span><br><span class="line"></span><br><span class="line">    if m.get_attr(&quot;NDA_LLADDR&quot;) is not None:</span><br><span class="line">      lladdr&#x3D;m.get_attr(&quot;NDA_LLADDR&quot;)</span><br><span class="line">      logging.info(&quot;L2Miss on &#123;&#125;: Who has Mac Address: &#123;&#125;?&quot;.format(ifname,lladdr))</span><br><span class="line"></span><br><span class="line">      (idx,answer)&#x3D;c.kv.get(consul_prefix+&quot;&#x2F;fib&#x2F;&quot;+lladdr)</span><br><span class="line">      if answer is not None:</span><br><span class="line">        dst_host&#x3D;answer[&quot;Value&quot;]</span><br><span class="line">        logging.info(&quot;Populating FIB table from Consul: MAC &#123;&#125; is on host &#123;&#125;&quot;.format(lladdr,dst_host))</span><br><span class="line">        try:</span><br><span class="line">           ipr.fdb(&#39;add&#39;,ifindex&#x3D;ifindex, lladdr&#x3D;lladdr, dst&#x3D;dst_host)</span><br><span class="line">        except NetlinkError as (code,message):</span><br><span class="line">            print(message)</span><br></pre></td></tr></table></figure> This full version of this script is also available on the github repository mentionned earlier. Here is a quick explanation of what it does:</p>
<p>Instead of processing <code>Netlink</code> messages manually, we use the pyroute2 library. This library will parse Netlink messages and allow us to send Netlink messages to configure ARP/FIB entries. In addition, we bind the Netlink socket in the overlay namespace. We could use the <code>ip netns</code> command to start the script in the namespace, but we also need to access Consul from the script to get configuration data. To achieve this, we will run the script in the host network namespace and bind the Netlink socket in the overlay namespace: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from pyroute2 import NetNS</span><br><span class="line">ipr &#x3D; NetNS(vxlan_ns)</span><br><span class="line">ipr.bind()</span><br><span class="line"></span><br><span class="line">c&#x3D;consul.Consul(host&#x3D;consul_host,port&#x3D;8500)</span><br></pre></td></tr></table></figure></p>
<p>We will now wait for <code>GETNEIGH</code> events: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while True:</span><br><span class="line">  msg&#x3D;ipr.get()</span><br><span class="line">  for m in msg:</span><br><span class="line">    if m[&#39;event&#39;] !&#x3D; &#39;RTM_GETNEIGH&#39;:</span><br><span class="line">      continue</span><br></pre></td></tr></table></figure></p>
<p>We retrieve the index of the interface and its name (for logging purposes): <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifindex&#x3D;m[&#39;ifindex&#39;]</span><br><span class="line">ifname&#x3D;ipr.get_links(ifindex)[0].get_attr(&quot;IFLA_IFNAME&quot;)</span><br></pre></td></tr></table></figure> Now, if the message is an L3 miss, we get the IP address from the Netlink message payload and try to look up the associated ARP entry from Consul. If we find it, we add the neighbor entry to the overlay namespace by sending a Netlink message to the kernel with the relevant information. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">if m.get_attr(&quot;NDA_DST&quot;) is not None:</span><br><span class="line">  ipaddr&#x3D;m.get_attr(&quot;NDA_DST&quot;)</span><br><span class="line">  logging.info(&quot;L3Miss on &#123;&#125;: Who has IP: &#123;&#125;?&quot;.format(ifname,ipaddr))</span><br><span class="line"></span><br><span class="line">  (idx,answer)&#x3D;c.kv.get(consul_prefix+&quot;&#x2F;arp&#x2F;&quot;+ipaddr)</span><br><span class="line">  if answer is not None:</span><br><span class="line">    mac_addr&#x3D;answer[&quot;Value&quot;]</span><br><span class="line">    logging.info(&quot;Populating ARP table from Consul: IP &#123;&#125; is &#123;&#125;&quot;.format(ipaddr,mac_addr))</span><br><span class="line">    try:</span><br><span class="line">        ipr.neigh(&#39;add&#39;, dst&#x3D;ipaddr, lladdr&#x3D;mac_addr, ifindex&#x3D;ifindex, state&#x3D;ndmsg.states[&#39;permanent&#39;])</span><br><span class="line">    except NetlinkError as (code,message):</span><br><span class="line">        print(message)</span><br></pre></td></tr></table></figure> If the message is an L2 miss, we do the same with the FIB data.</p>
<p>Let’s now try this script. First, we will clean up everything and recreate the overlay namespace and containers: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker rm -f $(docker ps -aq)</span><br><span class="line">docker0:~$ .&#x2F;create-overlay.sh</span><br><span class="line">docker0:~$ docker run -d --net&#x3D;none --name&#x3D;demo debian sleep 3600</span><br><span class="line">docker0:~$ .&#x2F;attach-ctn.sh demo 2</span><br><span class="line"></span><br><span class="line">docker1:~$ docker rm -f $(docker ps -aq)</span><br><span class="line">docker1:~$ .&#x2F;create-overlay.sh</span><br><span class="line">docker1:~$ docker run -d --net&#x3D;none --name&#x3D;demo debian sleep 3600</span><br><span class="line">docker1:~$ .&#x2F;attach-ctn.sh demo 3</span><br></pre></td></tr></table></figure> If we try to ping the container on docker1 from docker0, it will not work because we have no ARP/FIB data yet: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec -it demo ping -c 4 192.168.0.3</span><br><span class="line">PING 192.168.0.3 (192.168.0.3): 56 data bytes</span><br><span class="line">92 bytes from 192.168.0.2: Destination Host Unreachable</span><br><span class="line">92 bytes from 192.168.0.2: Destination Host Unreachable</span><br><span class="line">92 bytes from 192.168.0.2: Destination Host Unreachable</span><br><span class="line">92 bytes from 192.168.0.2: Destination Host Unreachable</span><br><span class="line">--- 192.168.0.3 ping statistics ---</span><br><span class="line">4 packets transmitted, 0 packets received, 100% packet loss</span><br></pre></td></tr></table></figure></p>
<p>We will now start our script on both hosts: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo python&#x2F;arpd-consul.py</span><br><span class="line">docker1:~$ sudo python&#x2F;arpd-consul.py</span><br></pre></td></tr></table></figure></p>
<p>And try pinging again (from another terminal on docker0): <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec -it demo ping -c 4 192.168.0.3</span><br><span class="line">PING 192.168.0.3 (192.168.0.3): 56 data bytes</span><br><span class="line">64 bytes from 192.168.0.3: icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;999.730 ms</span><br><span class="line">64 bytes from 192.168.0.3: icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;0.453 ms</span><br></pre></td></tr></table></figure> Here is the output we get the python script on docker0: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INFO Starting new HTTP connection (1): consul</span><br><span class="line">INFO L3Miss on vxlan1: Who has IP: 192.168.0.3?</span><br><span class="line">INFO Populating ARP table from Consul: IP 192.168.0.3 is 02:42:c0:a8:00:03</span><br><span class="line">INFO L2Miss on vxlan1: Who has Mac Address: 02:42:c0:a8:00:03?</span><br><span class="line">INFO Populating FIB table from Consul: MAC 02:42:c0:a8:00:03 is on host 10.0.0.11</span><br><span class="line">INFO L2Miss on vxlan1: Who has Mac Address: 02:42:c0:a8:00:03?</span><br><span class="line">INFO Populating FIB table from Consul: MAC 02:42:c0:a8:00:03 is on host 10.0.0.11</span><br></pre></td></tr></table></figure></p>
<p>First, we get an L3 miss (no ARP data for 192.168.0.3), we query Consul to find the Mac address and populate the neighbor table. Then we receive an L2 miss (no FIB information for 02:42:c0:a8:00:03), we look up this Mac address in Consul and populate the forwarding database.</p>
<p>On docker1, we see a similar output but we only get the L3 miss because the L2 forwarding data is learned by the overlay namespace when the ICMP request packet gets to the overlay.</p>
<p>Here is an overview of what we built: <img src="/.com//overlay_consul.png" alt="Dynamic overlay with Consul"><span class="image-caption">Dynamic overlay with Consul</span></p>
<h1 id="conclusion">Conclusion</h1>
<p>This concludes our three part blog post on the Docker overlay. Do not hesitate to ping me (on twitter for instance) if you see some mistakes/inaccuracies or if some part of the posts are not clear. I will do my best to amend these posts quickly.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/09/06/docker-overlay-net-deep-tuto-zn2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/06/docker-overlay-net-deep-tuto-zn2/" class="post-title-link" itemprop="url">深入了解 Docker overlay 网络机制2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-06 18:12:58" itemprop="dateCreated datePublished" datetime="2019-09-06T18:12:58+08:00">2019-09-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-09-07 00:37:58" itemprop="dateModified" datetime="2019-09-07T00:37:58+08:00">2019-09-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="introduction">Introduction</h1>
<p>In part 1 of this blog post we have seen how Docker creates a dedicated namespace for the overlay and connect the containers to this namespace. We have also seen that the overlay communication between Docker hosts uses VXLAN. In this second post, we will look at VXLAN in more details and how Docker uses it.</p>
<h1 id="what-is-vxlan">What is VXLAN ?</h1>
<p>From wikipedia:</p>
<blockquote>
<p>Virtual Extensible LAN (VXLAN) is a network virtualization technology that attempts to improve the scalability problems associated with large cloud computing deployments.</p>
</blockquote>
<p>VXLAN is a tunneling technology which encapsulates L2 frames inside UDP packets usually sent on port 4789. It was originally developed by VMware, Arista and Cisco. The main goal of VXLAN was to simplify cloud deployments which require multi-tenancy at the L2 layer. It provides:</p>
<ul>
<li>Tunneling L2 over L3 to avoid the necessity of L2 connectivity between all hosts in the cluster</li>
<li>More than 4096 isolated networks (VLAN IDs are limited to 4096)</li>
</ul>
<p>On Linux, Openvswitch supports VXLAN and the kernel has native support for it since version 3.7. In addition, VXLAN works with network namespaces since kernel 3.16.</p>
<p>Here is what a VXLAN packet looks like: <img src="/.com//vxlan-frame-layout.png" alt="VxLAN 数据包结构图"><span class="image-caption">VxLAN 数据包结构图</span></p>
<p>The <code>outer</code> IP packet is used for communication between hosts and the original L2 frame is encapuslated in a UDP packet with an additional VXLAN header for metadata (in particular the VXLAN ID).</p>
<p>We can verify that traffic between our hosts is using VXLAN with tcpdump. Let’s ping C0 from a container on docker1 and capture traffic on <code>docker0</code>: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ docker run -it --rm --net demonet debian ping 192.168.0.100</span><br><span class="line">PING 192.168.0.100 (192.168.0.100): 56 data bytes</span><br><span class="line">64 bytes from 192.168.0.100: icmp_seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.680 ms</span><br><span class="line">64 bytes from 192.168.0.100: icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.503 ms</span><br><span class="line"></span><br><span class="line">docker0:~$ sudo tcpdump -pni eth0 &quot;port 4789&quot;</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">12:55:53.652322 IP 10.0.0.11.64667 &gt; 10.0.0.10.4789: VXLAN, flags [I] (0x08), vni 256</span><br><span class="line">IP 192.168.0.2 &gt; 192.168.0.100: ICMP echo request, id 1, seq 0, length 64</span><br><span class="line"></span><br><span class="line">12:55:53.652409 IP 10.0.0.10.47697 &gt; 10.0.0.11.4789: VXLAN, flags [I] (0x08), vni 256</span><br><span class="line">IP 192.168.0.100 &gt; 192.168.0.2: ICMP echo reply, id 1, seq 0, length 64</span><br></pre></td></tr></table></figure></p>
<p>Each packet generates two lines of output in tcpdump because due to VXLAN frames analysis (a few fields have been removed for readability):</p>
<ul>
<li>the <code>outer</code> frame, with IP <code>10.0.0.11</code> and <code>10.0.0.10</code> (docker hosts)</li>
<li>the <code>inner</code> frame, with IP <code>192.168.0.100</code> and <code>192.168.0.2</code> (our containers) and an ICMP payload. We can also see the MAC addresses of our containers. <img src="/.com//connectivity-vxlanpacket.png" alt="Docker overlay 网络连接示意图"><span class="image-caption">Docker overlay 网络连接示意图</span></li>
</ul>
<h1 id="resolving-container-names-and-location">Resolving container names and location</h1>
<p>We have seen that we can ping containers on docker0 from containers on docker1 using VXLAN but we do not know yet how containers on each host can map IP addresses to MAC addresses and how the L2 frames are forwarded to the appropriate host.</p>
<p>Let’s create a container on docker1 and look at its ARP table: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ docker run -it --rm --net demonet debian bash</span><br><span class="line">root@6234b23677b9:&#x2F;# ip neighbor show</span><br></pre></td></tr></table></figure></p>
<p>There is no ARP information inside the container. If we ping C0 the container will generate ARP traffic. Let’s first see how this traffic is seen in the overlay namespace on docker0: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo nsenter --net&#x3D;$overns tcpdump -pni any &quot;arp&quot;</span><br></pre></td></tr></table></figure></p>
<p>Going back to our container, we will try to ping C0, which will generate an ARP packet: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@6234b23677b9:&#x2F;# ping 192.168.0.100</span><br></pre></td></tr></table></figure> There is nothing in tcpdump on docker0 so the ARP traffic is not sent in the VXLAN tunnel (you may see ARP requests but no for host 192.168.0.100). Let’s recreate a container on docker1 and tcpdump in the overlay namespace of docker1 to verify that we are getting ARP queries. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ docker run -it --rm --net demonet debian bash</span><br></pre></td></tr></table></figure> Let’s run tcpdump in another window. We list the Docker network namespaces to identify the namespace associated with the overlay. This namespace may change because the overlay namespace is deleted when there are no container attached to the network. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ sudo ls -1 &#x2F;var&#x2F;run&#x2F;docker&#x2F;netns</span><br><span class="line">102022d57fab</span><br><span class="line">x-13fb802253</span><br><span class="line">docker1:~$ overns&#x3D;&#x2F;var&#x2F;run&#x2F;docker&#x2F;netns&#x2F;x-13fb802253</span><br><span class="line">docker1:~$ sudo nsenter --net&#x3D;$overns tcpdump -peni any &quot;arp&quot;</span><br></pre></td></tr></table></figure></p>
<p>When we ping from the window with the container, here is what we see in tcpdump: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">19:16:40.658369 Out 02:42:c0:a8:00:02 ethertype ARP (0x0806), length 44: Request who-has 192.168.0.100 tell 192.168.0.2, length 28</span><br><span class="line">19:16:40.658352   B 02:42:c0:a8:00:02 ethertype ARP (0x0806), length 44: Request who-has 192.168.0.100 tell 192.168.0.2, length 28</span><br><span class="line">19:16:40.658371  In 02:42:c0:a8:00:64 ethertype ARP (0x0806), length 44: Reply 192.168.0.100 is-at 02:42:c0:a8:00:64, length 28</span><br><span class="line">19:16:40.658377 Out 02:42:c0:a8:00:64 ethertype ARP (0x0806), length 44: Reply 192.168.0.100 is-at 02:42:c0:a8:00:64, length 28</span><br></pre></td></tr></table></figure></p>
<p>We can see the ARP query and answer, which means the overlay namespace has the information and that it acts as an ARP proxy. We can easily verify this: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ sudo nsenter --net&#x3D;$overns ip neigh show</span><br><span class="line">192.168.0.100 dev vxlan0 lladdr 02:42:c0:a8:00:64 PERMANENT</span><br></pre></td></tr></table></figure></p>
<p>The entry is flagged as PERMANENT which means it is static and was “manually” added and not the result of an ARP discovery. What happens if we create a second container on docker0? <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker run -d --ip 192.168.0.200 --net demonet --name C1 debian sleep 3600</span><br><span class="line"></span><br><span class="line">docker1:~$ sudo nsenter --net&#x3D;$overns ip neigh show</span><br><span class="line">192.168.0.200 dev vxlan0 lladdr 02:42:c0:a8:00:c8 PERMANENT</span><br><span class="line">192.168.0.100 dev vxlan0 lladdr 02:42:c0:a8:00:64 PERMANENT</span><br></pre></td></tr></table></figure> The entry has been added automatically, even if no traffic was sent to this new container yet. This means that Docker is automatically populating the ARP entries in the overlay namespace and that the vxlan interface is acting as a proxy to answer ARP queries.</p>
<p>If we look at the configuration of the vxlan interface, we can see that it has the proxy flag set which explains this behavior (we will look at the other options later). <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ sudo nsenter --net&#x3D;$overns ip -d link show vxlan0</span><br><span class="line">xx: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master br0 state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 5a:71:8f:a4:b8:1b brd ff:ff:ff:ff:ff:ff promiscuity 1</span><br><span class="line">    vxlan id 256 srcport 10240 65535 dstport 4789 proxy l2miss l3miss ageing 300</span><br><span class="line">    bridge_slav</span><br></pre></td></tr></table></figure></p>
<p>What about the location of the MAC address (on which host is 02:42:c0:a8:00:64)? We can look at the bridge forwarding database in the overlay namespace: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ sudo nsenter --net&#x3D;$overns bridge fdb show</span><br><span class="line">5a:71:8f:a4:b8:1b dev vxlan0 vlan 0 master br0 permanent</span><br><span class="line">9a:ad:35:64:39:39 dev veth2 vlan 0 master br0 permanent</span><br><span class="line">02:42:c0:a8:00:c8 dev vxlan0 dst 10.0.0.10 self permanent</span><br><span class="line">02:42:c0:a8:00:64 dev vxlan0 dst 10.0.0.10 self permanent</span><br><span class="line">33:33:00:00:00:01 dev veth2 self permanent</span><br><span class="line">01:00:5e:00:00:01 dev veth2 self permanent</span><br></pre></td></tr></table></figure></p>
<p>We can see that the MAC addresses for our two containers on docker0 are in the database with a permanent flag. This information is also dynamically populated by Docker. <img src="/.com//connectivity-with-arp-fdb.png" alt="Docker overlay 网络连接示意图"><span class="image-caption">Docker overlay 网络连接示意图</span></p>
<h1 id="distribution-of-macfdb-information">Distribution of MAC/FDB information</h1>
<p>We have just discovered that Docker populates MAC and FDB information automatically. How is this done?</p>
<p>We can first look at the content of Consul. What is stored in there? <img src="/.com//consul-with-network.png" alt="Docker overlay 网络连接示意图"><span class="image-caption">Docker overlay 网络连接示意图</span></p>
<p>The network that was empty when we started now contains information and we can recognize the id of our overlay: <code>13fb802253b6f0a44e17e2b65505490e0c80527e1d78c4f5c74375aff4bf882a</code>.</p>
<p>The Consul UI does not display keys when they are too long but we can use curl to look at the content (Docker stores the information as JSON which is based64 encoded and Consul answers queries in JSON): <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">net&#x3D;$(docker network inspect demonet -f &#123;&#123;.Id&#125;&#125;)</span><br><span class="line">curl -s http:&#x2F;&#x2F;consul:8500&#x2F;v1&#x2F;kv&#x2F;docker&#x2F;network&#x2F;v1.0&#x2F;network&#x2F;$&#123;net&#125;&#x2F; | jq  -r &quot;.[0].Value&quot;  |  base64 -d | jq .</span><br><span class="line">&#123;</span><br><span class="line">  &quot;addrSpace&quot;: &quot;GlobalDefault&quot;,</span><br><span class="line">  &quot;attachable&quot;: false,</span><br><span class="line">  &quot;created&quot;: &quot;2017-04-23T16:33:02.442759329Z&quot;,</span><br><span class="line">  &quot;enableIPv6&quot;: false,</span><br><span class="line">  &quot;generic&quot;: &#123;</span><br><span class="line">    &quot;com.docker.network.enable_ipv6&quot;: false,</span><br><span class="line">    &quot;com.docker.network.generic&quot;: &#123;&#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;id&quot;: &quot;13fb802253b6f0a44e17e2b65505490e0c80527e1d78c4f5c74375aff4bf882a&quot;,</span><br><span class="line">  &quot;inDelete&quot;: false,</span><br><span class="line">  &quot;ingress&quot;: false,</span><br><span class="line">  &quot;internal&quot;: false,</span><br><span class="line">  &quot;ipamOptions&quot;: &#123;&#125;,</span><br><span class="line">  &quot;ipamType&quot;: &quot;default&quot;,</span><br><span class="line">  &quot;ipamV4Config&quot;: &quot;[&#123;\&quot;PreferredPool\&quot;:\&quot;192.168.0.0&#x2F;24\&quot;,\&quot;SubPool\&quot;:\&quot;\&quot;,\&quot;Gateway\&quot;:\&quot;\&quot;,\&quot;AuxAddresses\&quot;:null&#125;]&quot;,</span><br><span class="line">  &quot;ipamV4Info&quot;: &quot;[&#123;\&quot;IPAMData\&quot;:\&quot;&#123;\\\&quot;AddressSpace\\\&quot;:\\\&quot;GlobalDefault\\\&quot;,\\\&quot;Gateway\\\&quot;:\\\&quot;192.168.0.1&#x2F;24\\\&quot;,\\\&quot;Pool\\\&quot;:\\\&quot;192.168.0.0&#x2F;24\\\&quot;&#125;\&quot;,\&quot;PoolID\&quot;:\&quot;GlobalDefault&#x2F;192.168.0.0&#x2F;24\&quot;&#125;]&quot;,</span><br><span class="line">  &quot;labels&quot;: &#123;&#125;,</span><br><span class="line">  &quot;name&quot;: &quot;demonet&quot;,</span><br><span class="line">  &quot;networkType&quot;: &quot;overlay&quot;,</span><br><span class="line">  &quot;persist&quot;: true,</span><br><span class="line">  &quot;postIPv6&quot;: false,</span><br><span class="line">  &quot;scope&quot;: &quot;global&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> We can find all the metadata on our network:</p>
<ul>
<li>name: demonet</li>
<li>id: 13fb802253b6f0a44e17e2b65505490e0c80527e1d78c4f5c74375aff4bf882a</li>
<li>subnet range: 192.168.0.0/24</li>
</ul>
<p>We can also retrieve information about endpoints but the curl queries are hard to read so we will use this small python script (available on the GitHub repository) to retrieve this information: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import consul</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"># First we connect to consul</span><br><span class="line">c&#x3D;consul.Consul(host&#x3D;&quot;consul&quot;,port&#x3D;8500)</span><br><span class="line"></span><br><span class="line"># We retrieve all endpoint keys from Consul</span><br><span class="line">(idx,endpoints)&#x3D;c.kv.get(&quot;docker&#x2F;network&#x2F;v1.0&#x2F;endpoint&#x2F;&quot;,recurse&#x3D;True)</span><br><span class="line">epdata&#x3D;[ ep[&#39;Value&#39;] for ep in endpoints if ep[&#39;Value&#39;] is not None]</span><br><span class="line"></span><br><span class="line"># We print some interesting data on these endpoints</span><br><span class="line">for data in epdata:</span><br><span class="line">    jsondata&#x3D;json.loads(data.decode(&quot;utf-8&quot;))</span><br><span class="line">    print(&quot;Endpoint Name: %s&quot; % jsondata[&quot;name&quot;])</span><br><span class="line">    print(&quot;IP address: %s&quot; % jsondata[&quot;ep_iface&quot;][&quot;addr&quot;])</span><br><span class="line">    print(&quot;MAC address: %s&quot; % jsondata[&quot;ep_iface&quot;][&quot;mac&quot;])</span><br><span class="line">    print(&quot;Locator: %s\n&quot; % jsondata[&quot;locator&quot;])</span><br></pre></td></tr></table></figure></p>
<p>The script displays the main pieces of information on the container endpoints:</p>
<ul>
<li>Name</li>
<li>IP address</li>
<li>MAC address</li>
<li>Locator: the host where the container is located</li>
</ul>
<p>Here is what we find out about our setup: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ python&#x2F;dump_endpoints.py</span><br><span class="line">Endpoint Name: adoring_einstein</span><br><span class="line">IP address: 192.168.0.2&#x2F;24</span><br><span class="line">MAC address: 02:42:c0:a8:00:02</span><br><span class="line">Locator: 10.0.0.11</span><br><span class="line"></span><br><span class="line">Endpoint Name: C1</span><br><span class="line">IP address: 192.168.0.200&#x2F;24</span><br><span class="line">MAC address: 02:42:c0:a8:00:c8</span><br><span class="line">Locator: 10.0.0.10</span><br></pre></td></tr></table></figure></p>
<p>Consul is used as a reference store for all static information. However, it is not enough to dynamically notify all hosts when a container is created. It turns out that Docker uses Serf and its Gossip protocol to achieve this. We can easily verify this by subscribing to serf events on docker0 and create a container on docker1: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ serf agent -bind 0.0.0.0:17946 -join 10.0.0.11:7946 -node demo -log-level&#x3D;debug -event-handler&#x3D;.&#x2F;serf.sh</span><br><span class="line">#########################################</span><br><span class="line">New event: member-join</span><br><span class="line">demo    10.0.0.10</span><br><span class="line">docker0 10.0.0.10</span><br><span class="line">docker1 10.0.0.11</span><br><span class="line">#########################################</span><br></pre></td></tr></table></figure></p>
<p>I removed most of the output to focus on relevant information: we can see all the nodes participating in Gossip.</p>
<p>Serf is started with the following options:</p>
<ul>
<li>bind: to bind a port different from 7946 (already used by Docker)</li>
<li>join: to join the serf cluster</li>
<li>node: to give an alternate name to the node (docker0 is already taken)</li>
<li>event-handler: a simple script to display serf events</li>
<li>log-level=debug: required to see the output of the event handler script</li>
</ul>
<p>The serf.sh script has the following content: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;New event: $&#123;SERF_EVENT&#125;&quot;</span><br><span class="line">while read line; do</span><br><span class="line">    printf &quot;$&#123;line&#125;\n&quot;</span><br></pre></td></tr></table></figure></p>
<p>Let’s now create a container on docker1 and look at the ouput on docker0: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ docker run -it --rm --net demonet debian sleep 10</span><br></pre></td></tr></table></figure></p>
<p>On docker0 we see: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">New event: user</span><br><span class="line">join 192.168.0.2 255.255.255.0 02:42:c0:a8:00:02</span><br></pre></td></tr></table></figure></p>
<p>Then after 10s when the container exits on docker1: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">New event: user</span><br><span class="line">leave 192.168.0.2 255.255.255.0 02:42:c0:a8:00:02</span><br></pre></td></tr></table></figure></p>
<p>The Docker daemon subscribes to these events to create and remove entries in the ARP and FDB tables. <img src="/.com//connectivity-complete.png" alt="Docker overlay 网络连接示意图"><span class="image-caption">Docker overlay 网络连接示意图</span></p>
<p>In Swarm mode, Docker does not rely on Serf to synchronize information between nodes but relies on its own implementation of the Gossip protocol. It achieves exactly the same thing.</p>
<h1 id="alternate-vxlan-resolution-options">Alternate VXLAN resolution options</h1>
<p>The Docker daemon automatically populates ARP and FDB tables based on information received through the Gossip protocol via Serf, and relies on ARP proxying by the VLXAN interface. However, VXLAN also gives us other options for discovery.</p>
<h2 id="point-to-point-resolution">Point-to-point resolution</h2>
<p>When VXLAN is configured with the “remote » option, it sends all unknown traffic to this IP. This setup is very simple but limited to tunnels between two hosts. <img src="/.com//vxlan-point-to-point.png" alt="Docker overlay 网络连接示意图"><span class="image-caption">Docker overlay 网络连接示意图</span></p>
<h2 id="multicast-resolution">Multicast resolution</h2>
<p>When VXLAN is configured with the “group » option, it sends all unknown traffic to this multicast group. This setup is very efficient but requires multicast connectivity between all hosts, which is not always possible in particular when using public cloud. <img src="/.com//vxlan-multicast.png" alt="Docker overlay 网络连接示意图"><span class="image-caption">Docker overlay 网络连接示意图</span></p>
<p>For more detailed information on VXLAN configuration on Linux, I recommand this very complete post: VXLAN &amp; Linux.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In the first two parts of this post, we have seen how the Docker overlay works and the technologies it relies on. In the third and final part, we will see how we can build our own overlay from scratch using only Linux commands.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/09/06/docker-overlay-net-deep-tuto-zn1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/06/docker-overlay-net-deep-tuto-zn1/" class="post-title-link" itemprop="url">深入了解 Docker overlay 网络机制1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-06 10:58:26" itemprop="dateCreated datePublished" datetime="2019-09-06T10:58:26+08:00">2019-09-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-09-07 00:11:47" itemprop="dateModified" datetime="2019-09-07T00:11:47+08:00">2019-09-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>译者按： Laurent BERNAILLE 在他们公司的博客上写了三篇关于 Docker overlay network 工作机制的文章。本文因为有很高的专业度，所以一些英语表达，如 network overlay, linux network namespace, VxLAN 等并不翻译。</p>
<p><a href="https://blog.d2si.io/2017/04/25/deep-dive-into-docker-overlay-networks-part-1/" target="_blank" rel="noopener">点击此处阅读原文</a> Laurent BERNAILLE, 现任 Cloud Native Services CTO, Laurent est en perpétuelle réflexion sur les alternatives offertes par le Cloud Public. Laurent 帮助他的客户采用最新的 best pratiques 以及最信的技术，帮助客户们更快对市场做出反应。 Il oriente ses clients vers une adoption en continu des meilleurs pratiques et des dernières technologies pour qu'ils gagnent en vélocité.</p>
<h1 id="介绍">介绍</h1>
<p>在 D2SI(译者注：一家法国云服务创业公司), 我们自 Docker 诞生之初就一直在使用这个技术，并且一直在帮助众多项目迈入生产阶段。我们相信，使得一个项目步入生产阶段要求对项目使用的技术有很深入的理解，这样才有可能解决生产阶段中遇到的各种复杂的问题，分析遇到的异常行为抑或是解决性能下降。这就是为什么我们试着尽可能深入地去理解 Docker 使用的每个技术模块。</p>
<p>本文关注的问题是: Docker network overlay. Docker network overlay 驱动依赖以下技术：network namespace, VxLAN, Netlink 以及分布式键值存储系统(a distributed key-value store)。本文将逐一介绍上述机制以及他们对应的命令行工具。本来还将通过具体的操作展示， 当创建docker network overlay 以连接容器实例的时候，这些机制是如何彼此交互的。</p>
<p>本博文来源于我在在美国奥斯汀的 <a href="http://2017.dockercon.com/" target="_blank" rel="noopener">DockerCon2017</a> 上做的报告。演示文稿可以通过<a href="https://www.slideshare.net/lbernail/deep-dive-in-docker-overlay-networks" target="_blank" rel="noopener">点击链接</a>下载。本系列博文使用的所有代码均可通过<a href="https://github.com/lbernail/dockercon2017" target="_blank" rel="noopener">github</a>下载。</p>
<h1 id="docker-overlay-networks">Docker Overlay Networks</h1>
<p>首先，我们想在3台 Docker主机之间构建一个 overlay network，其中两台运行 Docker, 另外一台运行 Consul。本例中，Docker 使用 Consul 存储 overlay networks 相关的，需要在各个Docker主机之间共享的元数据, 例如，容器的IP地址，MAC地址以及位置(位于哪台Docker主机上)。在早于 Docker 1.12 版本, Docker 需要使用外置键值存储系统，如 Etcd or Consul，来创建 overlay network 以及 Docker Swarms (如今，这种方式创建的 Docker swarms 通过被称为 “古典 Swarm”)。从 Docker 1.12版本之后, Docker 可以采用内置的键值存储系统来创建 Docker 集群以及 overlay networks (“Swarm 模式” 或者 “新 swarm”)。本文中，我们选用 Consul，因为该技术允许我们查看由 Docker 存入的键值，让我们更好地理解键值存储系统的作用。我们让 Consule 运行在一个单节点上。在实际中，我们一般会用至少包含3个节点的集群，以保证可靠性。</p>
<p>在本文的示例中, 我们用的主机采用IP地址如下:</p>
<ul>
<li>consul: 10.0.0.5</li>
<li>docker0: 10.0.0.10</li>
<li>docker1: 10.0.0.0.11</li>
</ul>
<figure>
<img src="/.com//servers-network-topo.png" alt="基本网络拓扑结构示意图"><span class="image-caption">基本网络拓扑结构示意图</span><figcaption>基本网络拓扑结构示意图</figcaption>
</figure>
<h1 id="启动-consul-与-docker-服务">启动 Consul 与 Docker 服务</h1>
<p>首先我们需要启动 Consul 服务器。我们只需要从网上下载Consul。我们只需要通过下面的命令行开启一个最简约的 Consul 服务。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul agent -server -dev -ui -client 0.0.0.0</span><br></pre></td></tr></table></figure> 用到的参数解释如下：</p>
<ul>
<li>server: 以服务器模式启动 consul agent</li>
<li>dev: 创建一个没有任务持久化存储的单独的 Consul 服务器</li>
<li>ui: 开启 WEB 页面，以便查看由 Docker 创建的键值对</li>
<li>client 0.0.0.0: 为客户的访问绑定所有的网络接口。默认是只绑定<code>127.0.0.1</code></li>
</ul>
<p>为了配置 Docker 引擎使用 Consul 作为键值存储系统，需要启动 docker 守护进程，并且指定<code>cluster-store</code>选项: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dockerd -H fd:&#x2F;&#x2F; --cluster-store&#x3D;consul:&#x2F;&#x2F;consul:8500 --cluster-advertise&#x3D;eth0:2376</span><br></pre></td></tr></table></figure></p>
<p>可选参数 <code>cluster-advertise</code> 指定采用哪个IP地址在集群中to advertise in the cluster for a docker host (这个可选参数并不是可有可无). 上述命令假定<code>consul</code>可以被客户(即其他Docker主机)解析为了具体IP地址。在本例中，解析为<code>10.0.0.5</code>。</p>
<p>此时如果我们查看 Consul 图形界面，我们可以看到 Docker 确实创建一些键，但是网络键: <code>http://consul:8500/v1/kv/docker/network/v1.0/network/</code> 仍然是空的。</p>
<figure>
<img src="/.com//consul-start.png" alt="Consul启动后初始页面"><span class="image-caption">Consul启动后初始页面</span><figcaption>Consul启动后初始页面</figcaption>
</figure>
<p>你们可以很容易地在AWS平台上，通过使用terraform脚本(通过<a href="https://github.com/lbernail/dockercon2017" target="_blank" rel="noopener">Github下载</a>)来创建同样的实验环境。一切默认配置(尤其是要使用的区域)定义在<code>variables.tf</code>. 你们需要给变量<code>key_pair</code>赋值，可以通过命令行<code>terraform apply -var key_pair=demo</code>或者通过修改文件<code>variables.tf</code>。</p>
<p>这三个实例都被配置上了userdata: consul and docker 都被安装了并且通过合适的参数启动。一条记录被添加进文件<code>/etc/hosts</code>，以便 consul 可以被解析成 consul server 的IP地址。当连接到 consul or docker 服务器, 你们应该使用公共IP地址 (given in terraform outputs) and connect with user “admin” (the terraform setup uses a debian AMI).</p>
<h1 id="创建-overlay">创建 Overlay</h1>
<p>现在我们可以在两个Docker节点之间创建overlay网络: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker network create --driver overlay --subnet 192.168.0.0&#x2F;24 demonet</span><br><span class="line">13fb802253b6f0a44e17e2b65505490e0c80527e1d78c4f5c74375aff4bf882a</span><br></pre></td></tr></table></figure> 我们使用overlay驱动，选择<code>192.168.0.0/24</code>作为overlay的子网前缀 (<code>--subnet</code> 这个参数是可选的，但是我们希望配备一个显著不同于主机所在的子网前缀，让我们分析变得更简单)。</p>
<p>我们现在验证下overlay是否正确地在配置在了两台Docker节点上: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">eb096cb816c0        bridge              bridge              local</span><br><span class="line">13fb802253b6        demonet             overlay             global</span><br><span class="line">d538d58b17e7        host                host                local</span><br><span class="line">f2ee470bb968        none                null                local</span><br><span class="line"></span><br><span class="line">docker1:~$ docker network ls</span><br><span class="line">docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">eb7a05eba815        bridge              bridge              local</span><br><span class="line">13fb802253b6        demonet             overlay             global</span><br><span class="line">4346f6c422b2        host                host                local</span><br><span class="line">5e8ac997ecfa        none                null                local</span><br></pre></td></tr></table></figure> 看起来一切正常。两台Docker节点上均识别了标识为<code>demonet</code>的网络。<code>demonet</code>在两台Docker节点上有同样的ID。</p>
<p>现在我们来确认下创建的overlay网络可以正常运行。我们在节点docker0创建一个容器C0，显式地分配IP地址<code>192.168.0.100</code>, 并使之连接在overlay网络上。在另一个节点docker1上我们创建另键一个容器, 连接到同一个overlay网络中，并且使之ping下C0. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker run -d --ip 192.168.0.100 --net demonet --name C0 debian sleep 3600</span><br><span class="line"></span><br><span class="line">docker1:~$ docker run -it --rm --net demonet debian bash</span><br><span class="line">root@e37bf5e35f83:&#x2F;# ping 192.168.0.100</span><br><span class="line">PING 192.168.0.100 (192.168.0.100): 56 data bytes</span><br><span class="line">64 bytes from 192.168.0.100: icmp_seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.618 ms</span><br><span class="line">64 bytes from 192.168.0.100: icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.483 ms</span><br></pre></td></tr></table></figure> 关于 Docker 运行参数的一些解释:</p>
<ul>
<li>如果<code>docker run</code>后面追加<code>-d=true</code>或者<code>-d</code>，那么容器将会运行在后台模式。此时所有I/O数据只能通过网络资源或者共享卷组来进行交互。因为容器不再监听你执行<code>docker run</code>的这个终端命令行窗口。但你可以通过执行<code>docker attach</code>来重新附着到该容器的回话中。需要注意的是，容器运行在后台模式下，是不能使用<code>--rm</code>选项的。</li>
<li>如果要进行交互式操作（例如Shell脚本），那我们必须使用<code>-i -t</code>参数同容器进行数据交互。</li>
<li>默认情况下，每个容器在退出时，它的文件系统也会保存下来，这样一方面调试会方便些，因为你可以通过查看日志等方式来确定最终状态。另外一方面，你也可以保存容器所产生的数据。但是当你仅仅需要短暂的运行一个容器，并且这些数据不需要保存，你可能就希望Docker能在容器结束时自动清理其所产生的数据。这个时候你就需要<code>--rm</code>这个参数了。 注意：<code>--rm</code> 和 <code>-d</code> 不能共用！</li>
</ul>
<p>我们观察到，两个容器实例之间已经具备了连通性。如果我们试着从docker1(而不是一个容器实例)去ping容器C0, 是ping不同的，因为节点docker1并不清楚隔离在overlay中的子网<code>192.168.0.0/24</code>. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker1:~$ ping 192.168.0.100</span><br><span class="line">PING 192.168.0.100 (192.168.0.100) 56(84) bytes of data.</span><br><span class="line">^C--- 192.168.0.100 ping statistics ---</span><br><span class="line">4 packets transmitted, 0 received, 100% packet loss, time 3024ms</span><br></pre></td></tr></table></figure> Here is what we have built so far: <img src="/.com//overlay-net-topo-stage1.png"></p>
<h1 id="overlay-背后的原理">Overlay 背后的原理</h1>
<p>现在我们已经创建好了overlay网络。我们来看下docker overlay网络是如何工作的。</p>
<h2 id="容器的网络配置">容器的网络配置</h2>
<p>节点docker0上的容器C0的网络配置是怎样的？ <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec C0 ip addr show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default</span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1&#x2F;8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default</span><br><span class="line">    link&#x2F;ether 02:42:c0:a8:00:64 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.0.100&#x2F;24 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">9: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link&#x2F;ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.0.2&#x2F;16 scope global eth1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure> 该容器中，除loopback接口之外，我们还有两个网络接口：</p>
<ul>
<li>eth0: 配置了一个在子网192.168.0.0/24中的IP地址. 这个接口在overlay网络中(参加上图).</li>
<li>eth1: 配置了一个在子网172.18.0.2/16中的IP地址, 我们之前从未配置过该接口。</li>
</ul>
<p>那么C0的路由配置是怎样的？ <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec C0 ip route show</span><br><span class="line">default via 172.18.0.1 dev eth1</span><br><span class="line">172.18.0.0&#x2F;16 dev eth1  proto kernel  scope link  src 172.18.0.2</span><br><span class="line">192.168.0.0&#x2F;24 dev eth0  proto kernel  scope link  src 192.168.0.100</span><br></pre></td></tr></table></figure> 路由配置显示，默认路由是通过设备<code>eth1</code>. 这意味着，这个接口可以被用来访问overlay网络之外的资源。这一点可以通过ping一个外部IP地址来验证。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker exec -it C0 ping 8.8.8.8</span><br><span class="line">PING 8.8.8.8 (8.8.8.8): 56 data bytes</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq&#x3D;0 ttl&#x3D;51 time&#x3D;0.957 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq&#x3D;1 ttl&#x3D;51 time&#x3D;0.975 ms</span><br></pre></td></tr></table></figure></p>
<p>注意：我们也可以通过使用可选参数<code>--internal</code>创建一个overlay网络。这样该网络中的容器就无法访问外部网络。</p>
<p>我们来看看是否可以获得更多的关于这些网络接口的信息。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker exec C0 ip -details link show eth0</span><br><span class="line">6: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 02:42:c0:a8:00:64 brd ff:ff:ff:ff:ff:ff promiscuity 0</span><br><span class="line">    veth</span><br><span class="line"></span><br><span class="line">docker0:~$ docker exec C0 ip -details link show eth1</span><br><span class="line">9: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff promiscuity 0</span><br><span class="line">    veth</span><br></pre></td></tr></table></figure></p>
<p>两个网络接口的类型都是<code>veth</code>. <code>veth</code>类型的接口总是成对出现并通过一条虚拟的网线连接。这一对<code>veth</code>设备可以位于不同的network namespace，使得网络流量可以从一个namespace传递到另一个namespace. These two veth are used to get outside of the container network namespace.</p>
<p>Here is what we have found out so far:</p>
<p><img src="/.com//overlay-net-topo-stage2.png"></p>
<p>现在我们需要找出来<code>eth0</code>还有<code>eth1</code>的对等网络接口。</p>
<h2 id="what-is-the-container-connected-to">What is the container connected to?</h2>
<p>我们可以使用<code>ethtool</code>命令找到一个给定<code>veth</code>设备的另一端的对等设备。但是，<code>ethtool</code>这个命令在我们用的容器中并不可用。不过我们还是两种方式在容器中执行这个命令。第一个是用<code>nsenter</code>。<code>nsenter</code>允许我们进入一个或者多个和一个给定进程相关的命名空间。第二种方法是通过<code>ip netns exec</code>。这个命令依赖<code>iproute</code>在一个给定的network namespace执行命令。</p>
<p>Docker 不会在文件夹<code>/var/run/netns</code>中(<code>ip netns</code>查找network namespace的地方)创建符号链接。这就是为什么我们将依赖 <code>nsenter</code> (而不是<code>ip netns</code>)来查询由Docker创建的命名空间。</p>
<p>我们通过运行以下命令来列出来Docker创建的network namespace: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ls -1 &#x2F;var&#x2F;run&#x2F;docker&#x2F;netns</span><br><span class="line">e4b8ecb7ae7c</span><br><span class="line">1-13fb802253</span><br></pre></td></tr></table></figure> 为了使用这些信息，我们需要识别出容器的network namespace. 我们可以<code>inspect</code>指定容器，并且从<code>SandboxKey</code>中提取出我们需要的信息来做到。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker inspect C0 -f &#123;&#123;.NetworkSettings.SandboxKey&#125;&#125; </span><br><span class="line">&#x2F;var&#x2F;run&#x2F;docker&#x2F;netns&#x2F;e4b8ecb7ae7c</span><br><span class="line">docker0:~$ C0netns&#x3D;$(docker inspect C0 -f &#123;&#123;.NetworkSettings.SandboxKey&#125;&#125;)</span><br></pre></td></tr></table></figure></p>
<p>我们也可以在一个容器的network namespace中执行linux命令(即使容器没有这个命令): <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo nsenter --net&#x3D;$C0netns ip addr show eth0</span><br><span class="line">6: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default</span><br><span class="line">    link&#x2F;ether 02:42:c0:a8:00:64 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.0.100&#x2F;24 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure> 让我们看看和容器C0中的<code>eth0</code>还有<code>eth1</code>对等接口关联的网络接口的标识: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo nsenter --net&#x3D;$C0netns ethtool -S eth0</span><br><span class="line">NIC statistics:</span><br><span class="line">    peer_ifindex: 7</span><br><span class="line">docker0:~$ sudo nsenter --net&#x3D;$C0netns ethtool -S eth1</span><br><span class="line">NIC statistics:</span><br><span class="line">    peer_ifindex: 10</span><br></pre></td></tr></table></figure></p>
<p>我们现在再看看标识为<code>7</code>还有<code>10</code>网络接口。我们先在节点docker1上先查看一番: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ ip -details link show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 promiscuity 0</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link&#x2F;ether 06:e2:c0:20:ec:9f brd ff:ff:ff:ff:ff:ff promiscuity 0</span><br><span class="line">3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 02:42:a7:17:99:39 brd ff:ff:ff:ff:ff:ff promiscuity 0</span><br><span class="line">    bridge</span><br><span class="line">8: docker_gwbridge: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 02:42:be:d6:b0:c5 brd ff:ff:ff:ff:ff:ff promiscuity 0</span><br><span class="line">    bridge</span><br><span class="line">10: vethbc521fc: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 32:a1:47:1a:7f:1e brd ff:ff:ff:ff:ff:ff promiscuity 1</span><br><span class="line">    veth</span><br><span class="line">    bridge_slave</span><br></pre></td></tr></table></figure> 从该命令的输出来看，没有标识为<code>7</code>的接口的线索但是我们发现了标识为<code>10</code>的接口，也就是<code>eth1</code>的对等接口。另外, <code>10</code>号接口被连接在一个名为<code>docker_gwbridge</code>的网桥上。 <code>docker_gwbridge</code>网桥是个什么东东呢? 如果我们列出来由docker管理的网络，我们可以看到<code>docker_gwbridge</code>是出现在列表中的: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">eb096cb816c0        bridge              bridge              local</span><br><span class="line">13fb802253b6        demonet             overlay             global</span><br><span class="line">f6823b311fd2        docker_gwbridge     bridge              local</span><br><span class="line">d538d58b17e7        host                host                local</span><br><span class="line">f2ee470bb968        none                null                local</span><br></pre></td></tr></table></figure></p>
<p>现在我们来看网桥<code>docker_gwbridge</code>的元数据: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker inspect docker_gwbridge</span><br><span class="line">&quot;Name&quot;: &quot;docker_gwbridge&quot;,</span><br><span class="line">&quot;Driver&quot;: &quot;bridge&quot;,</span><br><span class="line">&quot;IPAM&quot;: &#123;</span><br><span class="line">    &quot;Driver&quot;: &quot;default&quot;,</span><br><span class="line">    &quot;Options&quot;: null,</span><br><span class="line">    &quot;Config&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;Subnet&quot;: &quot;172.18.0.0&#x2F;16&quot;,</span><br><span class="line">            &quot;Gateway&quot;: &quot;172.18.0.1&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;,</span><br><span class="line">&quot;Options&quot;: &#123;</span><br><span class="line">    &quot;com.docker.network.bridge.enable_icc&quot;: &quot;false&quot;,</span><br><span class="line">    &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;,</span><br><span class="line">    &quot;com.docker.network.bridge.name&quot;: &quot;docker_gwbridge&quot;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure> 我删掉了一些无关紧要的输出以便让大家关注重要的信心:</p>
<ul>
<li>this network uses the driver bridge (the same one used by the standard docker bridge, docker0)</li>
<li><code>it</code> uses subnet 172.18.0.0/16, which is consistent with eth1</li>
<li><code>enable_icc</code> is set to false which means we cannot use this bridge for inter-container communication</li>
<li><code>enable_ip_masquerade</code> is set to true, which means the traffic from the container will be NATed to access external networks (which we saw earlier when we successfully pinged 8.8.8.8)</li>
</ul>
<p>当从在docker0上的另一个也连接在<code>domenet</code>网络的容器去ping容器C0在<code>eth1</code>的IP地址 (172.18.0.2) 时，我们发现，容器间的通信是被禁止的。 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker run --rm -it --net demonet debian ping 172.18.0.2</span><br><span class="line">PING 172.18.0.2 (172.18.0.2): 56 data bytes</span><br><span class="line">^C--- 172.18.0.2 ping statistics ---</span><br><span class="line">3 packets transmitted, 0 packets received, 100% packet loss</span><br></pre></td></tr></table></figure></p>
<p>现在，我们又可以更新我们对Docker overlay网络的认知了: <img src="/.com//overlay-net-topo-stage3.png"></p>
<h2 id="what-about-eth0-the-interface-connected-to-the-overlay">What about eth0, the interface connected to the overlay?</h2>
<p><code>eth0</code>对等的网络接口并不在节点docker0的host network namespace. 那么它一定唯一另一个命名空间中。如果我们看看docker0上存在的网络空间: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ sudo ls -1 &#x2F;var&#x2F;run&#x2F;docker&#x2F;netns</span><br><span class="line">e4b8ecb7ae7c</span><br><span class="line">1-13fb802253</span><br></pre></td></tr></table></figure> 我们会发现一个名为<code>1-13fb802253</code>的命令空间。除了<code>1-</code>, 这个命名空间的名字正是我们overlay网络的网络ID的开始部分: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ docker network inspect demonet -f &#123;&#123;.Id&#125;&#125;</span><br><span class="line">13fb802253b6f0a44e17e2b65505490e0c80527e1d78c4f5c74375aff4bf882a</span><br></pre></td></tr></table></figure> 这个命名空间(<code>1-13fb802253</code>)很明显跟我们研究的overlay网络有关。让我们来看看这个命名空间拥有的网络接口： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">docker0:~$ overns&#x3D;&#x2F;var&#x2F;run&#x2F;docker&#x2F;netns&#x2F;1-13fb802253</span><br><span class="line">docker0:~$ sudo nsenter --net&#x3D;$overns ip -d link show</span><br><span class="line">2: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 3a:2d:44:c0:0e:aa brd ff:ff:ff:ff:ff:ff promiscuity 0</span><br><span class="line">    bridge</span><br><span class="line">5: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master br0 state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 4a:23:72:a3:fc:e3 brd ff:ff:ff:ff:ff:ff promiscuity 1</span><br><span class="line">    vxlan id 256 srcport 10240 65535 dstport 4789 proxy l2miss l3miss ageing 300</span><br><span class="line">    bridge_slave</span><br><span class="line">7: veth2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master br0 state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 3a:2d:44:c0:0e:aa brd ff:ff:ff:ff:ff:ff promiscuity 1</span><br><span class="line">    veth</span><br><span class="line">    bridge_slave</span><br></pre></td></tr></table></figure></p>
<p>overlay网络所在network namespace包含三个接口(以及lo)：</p>
<ul>
<li>br0: 网桥</li>
<li>veth2: <code>veth</code>类型网络接口，容器C0中网卡<code>eth0</code> 的对等接口，连接在bridge上。</li>
<li>vxlan0: <code>vxlan</code>类型网络接口，并且链接在网桥上。</li>
</ul>
<p>vxlan网络接口明显<code>overlay</code>发挥其魔力的地方。我们将看看其实现细节，不过让我们先更新下拓扑图。 <img src="/.com//overlay-net-topo-stage4.png"></p>
<h1 id="结论">结论</h1>
<p>本段是本系列文章第一部分的总结. 在第二部分, 我们将关注VxLAN: 什么是VxLAN以及Docker是如何使用VXLAN的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/09/05/docker-networking/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/05/docker-networking/" class="post-title-link" itemprop="url">Docker 网络教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-05 11:23:04" itemprop="dateCreated datePublished" datetime="2019-09-05T11:23:04+08:00">2019-09-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-10-08 18:12:22" itemprop="dateModified" datetime="2019-10-08T18:12:22+08:00">2019-10-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="容器网络">容器网络</h1>
<p>随着Docker的普及和使用，其自身的性能无法满足大规模集群的使用，需要一个工具对成千上万个容器进行统一编排。在2015年又开始了容器编排之争，行业内最主要的三个编排框架分别是docker公司的swarm、google的kubernetes以及Apache mesos。</p>
<p>Mesos是参考谷歌的borg大规模集群管理系统，并于2009年推出的。swarm和kubernetes是为docker等容器技术，新推出的框架。swarm是docker公司发布的，有近水楼台先得月的优势。kubernetes是参考谷歌的borg系统10几年的容器管理经验，重新推出的一套容器管理框架，可谓含着金钥匙出身，kubernetes迅速得到了微软，红帽等支持。这场战争不用打，或许都已经猜到结局是什么，战争的胜利只是一个时间问题。</p>
<p>2017年10月的dockerCon峰会上，docker公司官方支持Kubernetes，确立了kubernetes成为了容器编排界事实上的标准。</p>
<p>容器管理与编排平台 资源调度，服务发现，扩容缩容 * Docker 主推的 Swarm + Machine + Compose * Apache 提出的(Twitter主推)的 Mesos * Gooogle 推的开源的 Kubernetes</p>
<p>虚拟机网络与容器网络异同 * VM network 拥有完善的隔离机制，虚拟网卡与硬件网卡在使用上没有啥区别，而 container network 则使用 network namespace 提供网络在Linux kernel 中的隔离 * 处于安全考虑，很多情况下，容器会部署在VM内部，这种嵌套部署需要设计心得网络模型 * Container migration 速度很快，因此 network policy 的更新需要跟得上其速度 * 容器数量多了，多主机之间的 ARP flooding 会造成大量资源浪费</p>
<p>Docker 在开始的很长一段时间内只支持使用 <code>Linux bridge + iptables</code> 进行<code>single-host</code>的部署，自动化方面也只有 Pipework 这类 shell 脚本</p>
<figure>
<img src="/.com//docker-cnm.png" alt="Docker 提出的 container network model (CNM)"><span class="image-caption">Docker 提出的 container network model (CNM)</span><figcaption>Docker 提出的 container network model (CNM)</figcaption>
</figure>
<ul>
<li>Sandbox: isolated netwokring running environment. Save the essential container network stack configuration, including: network interface pairs, routing table and DNS configuration. Under linux, sandbox is implemnted in form of linux network namespace. A sanbox may include several endponts from several networks.</li>
<li>Endpoint: endpoint allows a sandbox to join in a network. Its implementation can be a veth pair or OVS internl port.</li>
<li>Network: Its implementation can be linux bridge, VLAN, etc.</li>
</ul>
<h1 id="libnetwork-实现了以下5中driver">Libnetwork 实现了以下5中driver:</h1>
<p>有两种方式来创建docker network：</p>
<ul>
<li>使用docker network create命令来创建，只能创建docker内建的网络模式</li>
<li>使用docker plugin，创建自定义网络 创建自定义网络需要设置网络的driver和ipam。</li>
</ul>
<p>Docker支持以下5中内建网络模式: ## Bridge: Docker默认的容器网络驱动。由Docker设计的一种NAT网络模型。Container通过一对veth pair连接到docker0网桥上，由Docker为容器动态分配IP及配置路由、防火墙规则等。Bridge模式下，Docker容器于Internet通信，以及不同容器之间的通信，都是通过iptables规则(如更换源或者目的地址等)控制的。 <img src="/.com//docker-bridge-mode.png" alt="Docker bridge mode"><span class="image-caption">Docker bridge mode</span></p>
<ul>
<li>Host: 容器与主机共享同一Network Namespace，共享同一套网络协议栈、路由表及iptables规则等。容器与主机看到的是相同的网络视图.</li>
<li>Null: 容器内网络配置为空，需要用户手动为容器配置网络接口及路由等。</li>
<li>Remote: Docker网络插件的实现。Remote driver使得Libnetwork可以通过HTTP RESTful API对接第三方的网络方案，类似SocketPlane的SDN方案只要实现了约定的HTTP URL处理函数及底层的网络接口配置方法，就可以替换Docker原生的网络实现(宋祁朋: 疑似在Docker官方文档中已经找不到这种模式了)。</li>
</ul>
<h2 id="overlay-mode">Overlay mode</h2>
<p>Docker原生的跨主机多子网网络方案。主要通过使用Linux bridge和vxlan隧道实现，底层通过类似于etcd或consul的KV存储系统实现多机的信息同步(宋祁朋：需要确认下)。The overlay network driver creates a distributed network among multiple Docker daemon hosts. This network sits on top of (overlays) the host-specific networks, allowing containers connected to it (including swarm service containers) to communicate securely. Docker transparently handles routing of each packet to and from the correct Docker daemon host and the correct destination container.</p>
<p>overlay网络模型比较复杂，底层需要类似consul或etcd的KV存储系统进行消息同步，核心是通过Linux网桥与vxlan隧道实现跨主机划分子网。每创建一个网络，Docker daemon 会在主机上创建一个单独的沙盒(i.e. network namespace). 在沙盒中，Docker(实际上是Docker daemon, 下同) 会创建名为br0的网桥，并在网桥上增加一个vxlan接口，每个网络占用一个<code>vxlan ID</code>，当前Docker创建vxlan隧道的ID范围为256~1000，因而最多可以创建745个网络。当添加一个容器到某一个网络上时，Docker会创建一对veth网卡设备，一端连接到此网络相关沙盒内的br0网桥上，另一端放入容器的沙盒内，并设置br0的IP地址作为容器内路由默认的网关地址，从而实现容器加入网络的目的。 <img src="/.com//docker-overlay-mode.png" alt="Docker overlay mode"><span class="image-caption">Docker overlay mode</span></p>
<p>总而言之，Docker的整个网络模型，是建立在Network Namespace、Linux网桥、vxlan隧道、iptables规则之上的，也正是由于过于依赖网桥与iptables，导致Docker的网络效率不高，招致了用户和开发者的诟病.</p>
<h1 id="appendix">Appendix</h1>
<h2 id="linux-network-namespace">Linux network namespace</h2>
<p>Linux 内核提供的一种网络资源虚拟隔离技术。不同network namespace具有各自的网络设备，协议栈，路由表以及防火墙规则等。同一network namespace下的进程则共享同一网络试图。Docker提供的5种网络模式均通过 network namespace实现。</p>
<h2 id="etcd">Etcd</h2>
<p>Etcd是CoreOS公司开发的开源的K-V存储及服务发现程序，有着大量的拥趸。在集群中，可用于在不同主机间交换配置、状态等信息，其功能强大，相应的配置也略微复杂一些。</p>
<h2 id="consul">Consul</h2>
<p>Consul is a fantastic solution for providing, among other things, powerful and reliable service-discovery capability to your network.</p>
<h1 id="kubernets">Kubernets</h1>
<h1 id="kubernets-和-docker-之间的关系">Kubernets 和 Docker 之间的关系</h1>
<p>Kubernetes与Docker有什么关系？ 众所周知，Docker提供容器的生命周期管理，Docker镜像构建运行时容器。但是，由于这些单独的容器必须通信，因此使用Kubernetes。因此，我们说Docker构建容器，这些容器通过Kubernetes相互通信。因此，可以使用Kubernetes手动关联和编排在多个主机上运行的容器。</p>
<p><code>helm</code> 是 Kubernets 生态中的一个包管理工具。helm chart 是将某个应用所需的所有配置等打包到一起，发布到 helm 的 repo 中，tiller 是 helm 的服务端组件，部署在k8s集群中。在部署应用时就可以使用类似于 <code>apt-get install</code> 的命令<code>helm install</code> 来安装。<code>helm</code> 相关组件的关系如下:</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://rennesong.com/2019/08/26/openstack-learning-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="rennsong">
      <meta itemprop="description" content="Personal blog site, to note technique and life related articles">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rennesong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/26/openstack-learning-note/" class="post-title-link" itemprop="url">每天5分钟玩转 OpenStack 读书笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-26 18:02:14" itemprop="dateCreated datePublished" datetime="2019-08-26T18:02:14+08:00">2019-08-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-08-29 23:03:29" itemprop="dateModified" datetime="2019-08-29T23:03:29+08:00">2019-08-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>KVM 狭义地来说，其实只是一个 linux 内核模块(<code>kvm.ko</code>), 负责虚拟机的调度以及内存管理。虚拟机的IO则有其他linux内核模块以及QEMU完成。</p>
<p><code>Libvirt</code> 简单说其实就是KVM的管理工具，不但是KVM，还提供了对 Xen, VirtualBox 的支持。<code>Libvirt</code> 包含 3 个东西：后台 daemon 程序 libvirtd、API 库和命令行工具 virsh</p>
<p>作为 KVM 和 OpenStack 的实施人员，virsh 和 virt-manager 是一定要会用的。</p>
<figure>
<img src="/.com//iaas-paas-saas.jpg" alt="云计算三种模式: IaaS, PaaS, SaaS"><span class="image-caption">云计算三种模式: IaaS, PaaS, SaaS</span><figcaption>云计算三种模式: IaaS, PaaS, SaaS</figcaption>
</figure>
<figure>
<img src="/.com//openstack-vm-creation-procedures.png" alt="OpenStack创建VM步骤示意图"><span class="image-caption">OpenStack创建VM步骤示意图</span><figcaption>OpenStack创建VM步骤示意图</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">rennsong</p>
  <div class="site-description" itemprop="description">Personal blog site, to note technique and life related articles</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">68</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hansomesong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hansomesong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">rennsong</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
